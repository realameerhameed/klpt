{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome / H\u00fbn bi x\u00ear hatin / \u0628\u06d5 \u062e\u06ce\u0631 \u0628\u06ce\u0646! \ud83d\ude42 Introduction Language technology is an increasingly important field in our information era which is dependent on our knowledge of the human language and computational methods to process it. Unlike the latter which undergoes constant progress with new methods and more efficient techniques being invented, the processability of human languages does not evolve with the same pace. This is particularly the case of languages with scarce resources and limited grammars, also known as less-resourced languages . Despite a plethora of performant tools and specific frameworks for natural language processing (NLP), such as NLTK , Stanza and spaCy , the progress with respect to less-resourced languages is often hindered by not only the lack of basic tools and resources but also the accessibility of the previous studies under an open-source licence. This is particularly the case of Kurdish. Kurdish Language Kurdish is a less-resourced Indo-European language which is spoken by 20-30 million speakers in the Kurdish regions of Turkey, Iraq, Iran and Syria and also, among the Kurdish diaspora around the world. It is mainly spoken in four dialects (also referred to as languages): Northern Kurdish (or Kurmanji) kmr Central Kurdish (or Sorani) ckb Southern Kurdish sdh Laki lki Kurdish has been historically written in various scripts, namely Cyrillic, Armenian, Latin and Arabic among which the latter two are still widely in use. Efforts in standardization of the Kurdish alphabets and orthographies have not succeeded to be globally followed by all Kurdish speakers in all regions. As such, the Kurmanji dialect is mostly written in the Latin-based script while the Sorani, Southern Kurdish and Laki are mostly written in the Arabic-based script. KLPT - The Kurdish Language Processing Toolkit KLPT - the Kurdish Language Processing Toolkit is an NLP toolkit for the Kurdish language . The current version (0.1) comes with four core modules, namely preprocess , stem , transliterate and tokenize , and addresses basic language processing tasks such as text preprocessing, stemming, tokenziation, spell error detection and correction, and morphological analysis for the Sorani and Kurmanji dialects of Kurdish. More importantly, it is an open-source project ! To find out more about how to use the tool, please check the \"User Guide\" section of this website. Cite this project Please consider citing this paper , if you use any part of the data or the tool ( bib file ): @inproceedings{ahmadi2020klpt, title = \"{KLPT} {--} {K}urdish Language Processing Toolkit\", author = \"Ahmadi, Sina\", booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\", month = nov, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2020.nlposs-1.11\", pages = \"72--84\" } You can also watch the presentation of this paper at https://slideslive.com/38939750/klpt-kurdish-language-processing-toolkit . License Kurdish Language Processing Toolkit by Sina Ahmadi is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License which means: You are free to share , copy and redistribute the material in any medium or format and also adapt, remix, transform, and build upon the material for any purpose, even commercially . You must give appropriate credit , provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original .","title":"Home"},{"location":"#welcome-hun-bi-xer-hatin","text":"","title":"Welcome / H\u00fbn bi x\u00ear hatin / \u0628\u06d5 \u062e\u06ce\u0631 \u0628\u06ce\u0646! \ud83d\ude42"},{"location":"#introduction","text":"Language technology is an increasingly important field in our information era which is dependent on our knowledge of the human language and computational methods to process it. Unlike the latter which undergoes constant progress with new methods and more efficient techniques being invented, the processability of human languages does not evolve with the same pace. This is particularly the case of languages with scarce resources and limited grammars, also known as less-resourced languages . Despite a plethora of performant tools and specific frameworks for natural language processing (NLP), such as NLTK , Stanza and spaCy , the progress with respect to less-resourced languages is often hindered by not only the lack of basic tools and resources but also the accessibility of the previous studies under an open-source licence. This is particularly the case of Kurdish.","title":"Introduction"},{"location":"#kurdish-language","text":"Kurdish is a less-resourced Indo-European language which is spoken by 20-30 million speakers in the Kurdish regions of Turkey, Iraq, Iran and Syria and also, among the Kurdish diaspora around the world. It is mainly spoken in four dialects (also referred to as languages): Northern Kurdish (or Kurmanji) kmr Central Kurdish (or Sorani) ckb Southern Kurdish sdh Laki lki Kurdish has been historically written in various scripts, namely Cyrillic, Armenian, Latin and Arabic among which the latter two are still widely in use. Efforts in standardization of the Kurdish alphabets and orthographies have not succeeded to be globally followed by all Kurdish speakers in all regions. As such, the Kurmanji dialect is mostly written in the Latin-based script while the Sorani, Southern Kurdish and Laki are mostly written in the Arabic-based script.","title":"Kurdish Language"},{"location":"#klpt-the-kurdish-language-processing-toolkit","text":"KLPT - the Kurdish Language Processing Toolkit is an NLP toolkit for the Kurdish language . The current version (0.1) comes with four core modules, namely preprocess , stem , transliterate and tokenize , and addresses basic language processing tasks such as text preprocessing, stemming, tokenziation, spell error detection and correction, and morphological analysis for the Sorani and Kurmanji dialects of Kurdish. More importantly, it is an open-source project ! To find out more about how to use the tool, please check the \"User Guide\" section of this website.","title":"KLPT - The Kurdish Language Processing Toolkit"},{"location":"#cite-this-project","text":"Please consider citing this paper , if you use any part of the data or the tool ( bib file ): @inproceedings{ahmadi2020klpt, title = \"{KLPT} {--} {K}urdish Language Processing Toolkit\", author = \"Ahmadi, Sina\", booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\", month = nov, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2020.nlposs-1.11\", pages = \"72--84\" } You can also watch the presentation of this paper at https://slideslive.com/38939750/klpt-kurdish-language-processing-toolkit .","title":"Cite this project"},{"location":"#license","text":"Kurdish Language Processing Toolkit by Sina Ahmadi is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License which means: You are free to share , copy and redistribute the material in any medium or format and also adapt, remix, transform, and build upon the material for any purpose, even commercially . You must give appropriate credit , provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use. If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original .","title":"License"},{"location":"about/contributing/","text":"How to help Kurdish language processing? One of our main objectives in this project is to promote collaborative projects with open-source outcomes. If you are generous and passionate to volunteer and help the Kurdish language, there are three ways you can do so: If you are a native Kurdish speaker with general knowledge about Kurdish and are comfortable working with computer, contributing to collaboratively-curated resources is the best starting point, particularly to: W\u00eek\u00eeferheng - the Kurdish Wiktionary Wikipedia in Sorani and in Kurmanji Tatoeba in Sorani and Kurmanji If you have expertise in Kurdish linguistics, you can take part in annotation tasks. Having a basic understanding on computational linguistics is a plus but not a must. Please get in touch by joining the KurdishNLP community on Gitter . Our collaborations oftentimes lead to a scientific paper depending on the task. Please check the following repositories to find out about some of our previous projects: Kurdish tokenization Kurdish Hunspell Kurdish transliteration If you are not included in 1 and 2 but have basic knowledge about Kurdish, particularly writing in Kurdish, you are invited to create content online. You can start creating a blog or tweet in Kurdish. After all, every single person is a contributor as well . In any case, please follow this project and introduce it to your friends. Test the tool and raise your issues so that we can fix them.","title":"Contributing"},{"location":"about/contributing/#how-to-help-kurdish-language-processing","text":"One of our main objectives in this project is to promote collaborative projects with open-source outcomes. If you are generous and passionate to volunteer and help the Kurdish language, there are three ways you can do so: If you are a native Kurdish speaker with general knowledge about Kurdish and are comfortable working with computer, contributing to collaboratively-curated resources is the best starting point, particularly to: W\u00eek\u00eeferheng - the Kurdish Wiktionary Wikipedia in Sorani and in Kurmanji Tatoeba in Sorani and Kurmanji If you have expertise in Kurdish linguistics, you can take part in annotation tasks. Having a basic understanding on computational linguistics is a plus but not a must. Please get in touch by joining the KurdishNLP community on Gitter . Our collaborations oftentimes lead to a scientific paper depending on the task. Please check the following repositories to find out about some of our previous projects: Kurdish tokenization Kurdish Hunspell Kurdish transliteration If you are not included in 1 and 2 but have basic knowledge about Kurdish, particularly writing in Kurdish, you are invited to create content online. You can start creating a blog or tweet in Kurdish. After all, every single person is a contributor as well . In any case, please follow this project and introduce it to your friends. Test the tool and raise your issues so that we can fix them.","title":"How to help Kurdish language processing?"},{"location":"about/license/","text":"License This project is created by Sina Ahmadi and is publicly available under a Creative Commons Attribution-ShareAlike 4.0 International Public License https://creativecommons.org/licenses/by-sa/4.0/ .","title":"License"},{"location":"about/license/#license","text":"This project is created by Sina Ahmadi and is publicly available under a Creative Commons Attribution-ShareAlike 4.0 International Public License https://creativecommons.org/licenses/by-sa/4.0/ .","title":"License"},{"location":"about/release-notes/","text":"About the current version Please note that KLPT is under development and some of the functionalities will appear in the future versions. You can find out regarding the progress of each task at the Projects section. In the current version, the following tasks are included: Modules Tasks Sorani (ckb) Kurmanji (kmr) preprocess normalization \u2713 (v0.1.0) \u2713 (v0.1.0) standardization \u2713 (v0.1.0) \u2713 (v0.1.0) unification of numerals \u2713 (v0.1.0) \u2713 (v0.1.0) tokenize word tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) MWE tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) sentence tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) transliterate Arabic to Latin \u2713 (v0.1.0) \u2713 (v0.1.0) Latin to Arabic \u2713 (v0.1.0) \u2713 (v0.1.0) Detection of u/w and \u00ee/y \u2713 (v0.1.0) \u2713 (v0.1.0) Detection of Bizroke ( i ) \u2717 \u2717 stem morphological analysis \u2713 (v0.1.0) \u2717 morphological generation \u2713 (v0.1.0) \u2717 stemming \u2717 \u2717 lemmatization \u2717 \u2717 spell error detection and correction \u2713 (v0.1.0) \u2717","title":"Release Notes"},{"location":"about/release-notes/#about-the-current-version","text":"Please note that KLPT is under development and some of the functionalities will appear in the future versions. You can find out regarding the progress of each task at the Projects section. In the current version, the following tasks are included: Modules Tasks Sorani (ckb) Kurmanji (kmr) preprocess normalization \u2713 (v0.1.0) \u2713 (v0.1.0) standardization \u2713 (v0.1.0) \u2713 (v0.1.0) unification of numerals \u2713 (v0.1.0) \u2713 (v0.1.0) tokenize word tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) MWE tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) sentence tokenization \u2713 (v0.1.0) \u2713 (v0.1.0) transliterate Arabic to Latin \u2713 (v0.1.0) \u2713 (v0.1.0) Latin to Arabic \u2713 (v0.1.0) \u2713 (v0.1.0) Detection of u/w and \u00ee/y \u2713 (v0.1.0) \u2713 (v0.1.0) Detection of Bizroke ( i ) \u2717 \u2717 stem morphological analysis \u2713 (v0.1.0) \u2717 morphological generation \u2713 (v0.1.0) \u2717 stemming \u2717 \u2717 lemmatization \u2717 \u2717 spell error detection and correction \u2713 (v0.1.0) \u2717","title":"About the current version"},{"location":"about/sponsors/","text":"Become a sponsor Please consider donating to the project. Data annotation and resource creation requires tremendous amount of time and linguistic expertise. Even a trivial donation will make a difference. You can do so by becoming a sponsor to accompany me in this journey and help the Kurdish language have a better place within other natural languages on the Web. Depending on your support, You can be an official sponsor You will get a GitHub sponsor badge on your profile If you have any questions, I will focus on it If you want, I will add your name or company logo on the front page of your preferred project Your contribution will be acknowledged in one of my future papers in a field of your choice Our sponsors: Be the first one! \ud83d\ude42 Name/company donation ($) URL","title":"Sponsors"},{"location":"about/sponsors/#become-a-sponsor","text":"Please consider donating to the project. Data annotation and resource creation requires tremendous amount of time and linguistic expertise. Even a trivial donation will make a difference. You can do so by becoming a sponsor to accompany me in this journey and help the Kurdish language have a better place within other natural languages on the Web. Depending on your support, You can be an official sponsor You will get a GitHub sponsor badge on your profile If you have any questions, I will focus on it If you want, I will add your name or company logo on the front page of your preferred project Your contribution will be acknowledged in one of my future papers in a field of your choice","title":"Become a sponsor"},{"location":"about/sponsors/#our-sponsors","text":"Be the first one! \ud83d\ude42 Name/company donation ($) URL","title":"Our sponsors:"},{"location":"user-guide/getting-started/","text":"Install KLPT KLPT is implemented in Python and requires basic knowledge on programming and particularly the Python language. Find out more about Python at https://www.python.org/ . Requirements Operating system : macOS / OS X \u00b7 Linux \u00b7 Windows (Cygwin, MinGW, Visual Studio) Python version : Python 3.5+ Package managers : pip cyhunspell >= 2.0.1 pip Using pip, KLPT releases are available as source packages and binary wheels. Please make sure that a compatible Python version is installed: pip install klpt All the data files including lexicons and morphological rules are also installed with the package. Although KLPT is not dependent on any NLP toolkit, there is one important requirement, particularly for the stem module. That is cyhunspell which should be installed with a version >= 2.0.1. Import klpt Once the package is installed, you can import the toolkit as follows: import klpt As a principle, the following parameters are widely used in the toolkit: dialect : the name of the dialect as Sorani or Kurmanji (ISO 639-3 code will be also added) script : the script of your input text as \"Arabic\" or \"Latin\" numeral : the type of the numerals as Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] Latin [1234567890]","title":"Getting started"},{"location":"user-guide/getting-started/#install-klpt","text":"KLPT is implemented in Python and requires basic knowledge on programming and particularly the Python language. Find out more about Python at https://www.python.org/ .","title":"Install KLPT"},{"location":"user-guide/getting-started/#requirements","text":"Operating system : macOS / OS X \u00b7 Linux \u00b7 Windows (Cygwin, MinGW, Visual Studio) Python version : Python 3.5+ Package managers : pip cyhunspell >= 2.0.1","title":"Requirements"},{"location":"user-guide/getting-started/#pip","text":"Using pip, KLPT releases are available as source packages and binary wheels. Please make sure that a compatible Python version is installed: pip install klpt All the data files including lexicons and morphological rules are also installed with the package. Although KLPT is not dependent on any NLP toolkit, there is one important requirement, particularly for the stem module. That is cyhunspell which should be installed with a version >= 2.0.1.","title":"pip"},{"location":"user-guide/getting-started/#import-klpt","text":"Once the package is installed, you can import the toolkit as follows: import klpt As a principle, the following parameters are widely used in the toolkit: dialect : the name of the dialect as Sorani or Kurmanji (ISO 639-3 code will be also added) script : the script of your input text as \"Arabic\" or \"Latin\" numeral : the type of the numerals as Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] Latin [1234567890]","title":"Import klpt"},{"location":"user-guide/preprocess/","text":"preprocess package This module deals with normalizing scripts and orthographies by using writing conventions based on dialects and scripts. The goal is not to correct the orthography but to normalize the text in terms of the encoding and common writing rules. The input encoding should be in UTF-8 only. To this end, three functions are provided as follows: normalize : deals with different encodings and unifies characters based on dialects and scripts standardize : given a normalized text, it returns standardized text based on the Kurdish orthographies following recommendations for Kurmanji and Sorani unify_numerals : conversion of the various types of numerals used in Kurdish texts preprocess : one single function for normalization, standardization and unification of numerals In addition, it is possible to remove stopwords using the stopwords variable. It is better to remove stopwords after the tokenization task. It is recommended that the output of this module be used as the input of subsequent tasks in an NLP pipeline. Examples: >>> from klpt.preprocess import Preprocess >>> preprocessor_ckb = Preprocess(\"Sorani\", \"Arabic\", numeral=\"Latin\") >>> preprocessor_ckb.normalize(\"\u0644\u06d5 \u0633\u0640\u0640\u0640\u0627\u06b5\u06d5\u06a9\u0627\u0646\u06cc \u0661\u0669\u0665\u0660\u062f\u0627\") '\u0644\u06d5 \u0633\u0627\u06b5\u06d5\u06a9\u0627\u0646\u06cc 1950\u062f\u0627' >>> preprocessor_ckb.standardize(\"\u0631\u0627\u0633\u062a\u06d5 \u0644\u06d5\u0648 \u0648\u0648\u06b5\u0627\u062a\u06d5\u062f\u0627\") '\u0695\u0627\u0633\u062a\u06d5 \u0644\u06d5\u0648 \u0648\u06b5\u0627\u062a\u06d5\u062f\u0627' >>> preprocessor_ckb.unify_numerals(\"\u0662\u0660\u0662\u0660\") '2020' >>> preprocessor_ckb.preprocess(\"\u0631\u0627\u0633\u062a\u06d5 \u0644\u06d5 \u0648\u0648\u06b5\u0627\u062a\u06d5\u06cc \u0662\u0663\u0647\u06d5\u0645\u062f\u0627\") '\u0695\u0627\u0633\u062a\u06d5 \u0644\u06d5 \u0648\u06b5\u0627\u062a\u06d5\u06cc 23\u0647\u06d5\u0645\u062f\u0627' >>> preprocessor_kmr = Preprocess(\"Kurmanji\", \"Latin\") >>> preprocessor_kmr.standardize(\"di sala 2018-an\") 'di sala 2018an' >>> preprocessor_kmr.standardize(\"h\u00eaviya\") 'h\u00eav\u00eeya' >>> preprocessor_kmr.stopwords[:10] ['a', 'an', 'bareya', 'barey\u00ea', 'bar\u00ean', 'basa', 'be', 'bel\u00ea', 'ber', 'bereya'] The preprocessing rules are provided at data/preprocess_map.json . __init__ ( self , dialect , script , numeral = 'Latin' ) special Initialization of the Preprocess class Parameters: Name Type Description Default dialect str the name of the dialect or its ISO 639-3 code required script str the name of the script required numeral str the type of the numeral 'Latin' Source code in klpt/preprocess.py def __init__ ( self , dialect , script , numeral = \"Latin\" ): \"\"\" Initialization of the Preprocess class Arguments: dialect (str): the name of the dialect or its ISO 639-3 code script (str): the name of the script numeral (str): the type of the numeral \"\"\" with open ( klpt . get_data ( \"data/preprocess_map.json\" ), encoding = \"utf-8\" ) as preprocess_file : self . preprocess_map = json . load ( preprocess_file ) configuration = Configuration ({ \"dialect\" : dialect , \"script\" : script , \"numeral\" : numeral }) self . dialect = configuration . dialect self . script = configuration . script self . numeral = configuration . numeral # self.preprocess_map = config.preprocess_map with open ( klpt . data_directory [ \"stopwords\" ], \"r\" , encoding = \"utf-8\" ) as f : self . stopwords = json . load ( f )[ dialect ][ script ] normalize ( self , text ) Text normalization This function deals with different encodings and unifies characters based on dialects and scripts as follows: Sorani-Arabic: replace frequent Arabic characters with their equivalent Kurdish ones, e.g. \"\u064a\" by \"\u06cc\" and \"\u0643\" by \"\u06a9\" replace \"\u0647\" followed by zero-width non-joiner (ZWNJ, U+200C) with \"\u06d5\" where ZWNJ is removed (\"\u0631\u0647\u200c\u0632\u0628\u0647\u200c\u0631\" is converted to \"\u0631\u06d5\u0632\u0628\u06d5\u0631\"). ZWNJ in HTML is also taken into account. replace \"\u0647\u0640\" with \"\u06be\" (U+06BE, ARABIC LETTER HEH DOACHASHMEE) remove Kashida \"\u0640\" \"\u06be\" in the middle of a word is replaced by \u0647 (U+0647) replace different types of y, such as 'ARABIC LETTER ALEF MAKSURA' (U+0649) It should be noted that the order of the replacements is important. Check out provided files for further details and test cases. Parameters: Name Type Description Default text str a string required Returns: Type Description str normalized text Source code in klpt/preprocess.py def normalize ( self , text ): \"\"\" Text normalization This function deals with different encodings and unifies characters based on dialects and scripts as follows: - Sorani-Arabic: - replace frequent Arabic characters with their equivalent Kurdish ones, e.g. \"\u064a\" by \"\u06cc\" and \"\u0643\" by \"\u06a9\" - replace \"\u0647\" followed by zero-width non-joiner (ZWNJ, U+200C) with \"\u06d5\" where ZWNJ is removed (\"\u0631\u0647\u200c\u0632\u0628\u0647\u200c\u0631\" is converted to \"\u0631\u06d5\u0632\u0628\u06d5\u0631\"). ZWNJ in HTML is also taken into account. - replace \"\u0647\u0640\" with \"\u06be\" (U+06BE, ARABIC LETTER HEH DOACHASHMEE) - remove Kashida \"\u0640\" - \"\u06be\" in the middle of a word is replaced by \u0647 (U+0647) - replace different types of y, such as 'ARABIC LETTER ALEF MAKSURA' (U+0649) It should be noted that the order of the replacements is important. Check out provided files for further details and test cases. Arguments: text (str): a string Returns: str: normalized text \"\"\" temp_text = \" \" + self . unify_numerals ( text ) + \" \" for normalization_type in [ \"universal\" , self . dialect ]: for rep in self . preprocess_map [ \"normalizer\" ][ normalization_type ][ self . script ]: rep_tar = self . preprocess_map [ \"normalizer\" ][ normalization_type ][ self . script ][ rep ] temp_text = re . sub ( rf \" { rep } \" , rf \" { rep_tar } \" , temp_text , flags = re . I ) return temp_text . strip () preprocess ( self , text ) One single function for normalization, standardization and unification of numerals Parameters: Name Type Description Default text str a string required Returns: Type Description str preprocessed text Source code in klpt/preprocess.py def preprocess ( self , text ): \"\"\" One single function for normalization, standardization and unification of numerals Arguments: text (str): a string Returns: str: preprocessed text \"\"\" return self . unify_numerals ( self . standardize ( self . normalize ( text ))) standardize ( self , text ) Method of standardization of Kurdish orthographies Given a normalized text, it returns standardized text based on the Kurdish orthographies. Sorani-Arabic: replace alveolar flap \u0631 (/\u027e/) at the begging of the word by the alveolar trill \u0695 (/r/) replace double rr and ll with \u0159 and \u0142 respectively Kurmanji-Latin: replace \"-an\" or \"'an\" in dates and numerals (\"di sala 2018'an\" and \"di sala 2018-an\" -> \"di sala 2018an\") Open issues: - replace \" \u0648\u06d5 \" by \" \u0648 \"? But this is not always possible, \"min bo we\" (\u0631\u06cc\u0632\u06af\u0640\u0631\u062a\u0646\u0627 \u0645\u0646 \u0628\u0648 \u0648\u06d5 \u0646\u06d5 \u0626\u06d5 \u0648\u06d5 \u0626\u0640\u0640\u06d5 \u0632) - \"pirt\u00fck\u00ea\": \"pirt\u00fbk\u00ea\"? - Should \u0131 (LATIN SMALL LETTER DOTLESS I be replaced by i? Parameters: Name Type Description Default text str a string required Returns: Type Description str standardized text Source code in klpt/preprocess.py def standardize ( self , text ): \"\"\" Method of standardization of Kurdish orthographies Given a normalized text, it returns standardized text based on the Kurdish orthographies. - Sorani-Arabic: - replace alveolar flap \u0631 (/\u027e/) at the begging of the word by the alveolar trill \u0695 (/r/) - replace double rr and ll with \u0159 and \u0142 respectively - Kurmanji-Latin: - replace \"-an\" or \"'an\" in dates and numerals (\"di sala 2018'an\" and \"di sala 2018-an\" -> \"di sala 2018an\") Open issues: - replace \" \u0648\u06d5 \" by \" \u0648 \"? But this is not always possible, \"min bo we\" (\u0631\u06cc\u0632\u06af\u0640\u0631\u062a\u0646\u0627 \u0645\u0646 \u0628\u0648 \u0648\u06d5 \u0646\u06d5 \u0626\u06d5 \u0648\u06d5 \u0626\u0640\u0640\u06d5 \u0632) - \"pirt\u00fck\u00ea\": \"pirt\u00fbk\u00ea\"? - Should [\u0131 (LATIN SMALL LETTER DOTLESS I](https://www.compart.com/en/unicode/U+0131) be replaced by i? Arguments: text (str): a string Returns: str: standardized text \"\"\" temp_text = \" \" + self . unify_numerals ( text ) + \" \" for standardization_type in [ self . dialect ]: for rep in self . preprocess_map [ \"standardizer\" ][ standardization_type ][ self . script ]: rep_tar = self . preprocess_map [ \"standardizer\" ][ standardization_type ][ self . script ][ rep ] temp_text = re . sub ( rf \" { rep } \" , rf \" { rep_tar } \" , temp_text , flags = re . I ) return temp_text . strip () unify_numerals ( self , text ) Convert numerals to the desired one There are three types of numerals: - Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] - Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] - Latin [1234567890] Parameters: Name Type Description Default text str a string required Returns: Type Description str text with unified numerals Source code in klpt/preprocess.py def unify_numerals ( self , text ): \"\"\" Convert numerals to the desired one There are three types of numerals: - Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] - Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] - Latin [1234567890] Arguments: text (str): a string Returns: str: text with unified numerals \"\"\" for i , j in self . preprocess_map [ \"normalizer\" ][ \"universal\" ][ \"numerals\" ][ self . numeral ] . items (): text = text . replace ( i , j ) return text","title":"Preprocess"},{"location":"user-guide/preprocess/#preprocess-package","text":"This module deals with normalizing scripts and orthographies by using writing conventions based on dialects and scripts. The goal is not to correct the orthography but to normalize the text in terms of the encoding and common writing rules. The input encoding should be in UTF-8 only. To this end, three functions are provided as follows: normalize : deals with different encodings and unifies characters based on dialects and scripts standardize : given a normalized text, it returns standardized text based on the Kurdish orthographies following recommendations for Kurmanji and Sorani unify_numerals : conversion of the various types of numerals used in Kurdish texts preprocess : one single function for normalization, standardization and unification of numerals In addition, it is possible to remove stopwords using the stopwords variable. It is better to remove stopwords after the tokenization task. It is recommended that the output of this module be used as the input of subsequent tasks in an NLP pipeline. Examples: >>> from klpt.preprocess import Preprocess >>> preprocessor_ckb = Preprocess(\"Sorani\", \"Arabic\", numeral=\"Latin\") >>> preprocessor_ckb.normalize(\"\u0644\u06d5 \u0633\u0640\u0640\u0640\u0627\u06b5\u06d5\u06a9\u0627\u0646\u06cc \u0661\u0669\u0665\u0660\u062f\u0627\") '\u0644\u06d5 \u0633\u0627\u06b5\u06d5\u06a9\u0627\u0646\u06cc 1950\u062f\u0627' >>> preprocessor_ckb.standardize(\"\u0631\u0627\u0633\u062a\u06d5 \u0644\u06d5\u0648 \u0648\u0648\u06b5\u0627\u062a\u06d5\u062f\u0627\") '\u0695\u0627\u0633\u062a\u06d5 \u0644\u06d5\u0648 \u0648\u06b5\u0627\u062a\u06d5\u062f\u0627' >>> preprocessor_ckb.unify_numerals(\"\u0662\u0660\u0662\u0660\") '2020' >>> preprocessor_ckb.preprocess(\"\u0631\u0627\u0633\u062a\u06d5 \u0644\u06d5 \u0648\u0648\u06b5\u0627\u062a\u06d5\u06cc \u0662\u0663\u0647\u06d5\u0645\u062f\u0627\") '\u0695\u0627\u0633\u062a\u06d5 \u0644\u06d5 \u0648\u06b5\u0627\u062a\u06d5\u06cc 23\u0647\u06d5\u0645\u062f\u0627' >>> preprocessor_kmr = Preprocess(\"Kurmanji\", \"Latin\") >>> preprocessor_kmr.standardize(\"di sala 2018-an\") 'di sala 2018an' >>> preprocessor_kmr.standardize(\"h\u00eaviya\") 'h\u00eav\u00eeya' >>> preprocessor_kmr.stopwords[:10] ['a', 'an', 'bareya', 'barey\u00ea', 'bar\u00ean', 'basa', 'be', 'bel\u00ea', 'ber', 'bereya'] The preprocessing rules are provided at data/preprocess_map.json .","title":"preprocess package"},{"location":"user-guide/preprocess/#klpt.preprocess.Preprocess.__init__","text":"Initialization of the Preprocess class Parameters: Name Type Description Default dialect str the name of the dialect or its ISO 639-3 code required script str the name of the script required numeral str the type of the numeral 'Latin' Source code in klpt/preprocess.py def __init__ ( self , dialect , script , numeral = \"Latin\" ): \"\"\" Initialization of the Preprocess class Arguments: dialect (str): the name of the dialect or its ISO 639-3 code script (str): the name of the script numeral (str): the type of the numeral \"\"\" with open ( klpt . get_data ( \"data/preprocess_map.json\" ), encoding = \"utf-8\" ) as preprocess_file : self . preprocess_map = json . load ( preprocess_file ) configuration = Configuration ({ \"dialect\" : dialect , \"script\" : script , \"numeral\" : numeral }) self . dialect = configuration . dialect self . script = configuration . script self . numeral = configuration . numeral # self.preprocess_map = config.preprocess_map with open ( klpt . data_directory [ \"stopwords\" ], \"r\" , encoding = \"utf-8\" ) as f : self . stopwords = json . load ( f )[ dialect ][ script ]","title":"__init__()"},{"location":"user-guide/preprocess/#klpt.preprocess.Preprocess.normalize","text":"Text normalization This function deals with different encodings and unifies characters based on dialects and scripts as follows: Sorani-Arabic: replace frequent Arabic characters with their equivalent Kurdish ones, e.g. \"\u064a\" by \"\u06cc\" and \"\u0643\" by \"\u06a9\" replace \"\u0647\" followed by zero-width non-joiner (ZWNJ, U+200C) with \"\u06d5\" where ZWNJ is removed (\"\u0631\u0647\u200c\u0632\u0628\u0647\u200c\u0631\" is converted to \"\u0631\u06d5\u0632\u0628\u06d5\u0631\"). ZWNJ in HTML is also taken into account. replace \"\u0647\u0640\" with \"\u06be\" (U+06BE, ARABIC LETTER HEH DOACHASHMEE) remove Kashida \"\u0640\" \"\u06be\" in the middle of a word is replaced by \u0647 (U+0647) replace different types of y, such as 'ARABIC LETTER ALEF MAKSURA' (U+0649) It should be noted that the order of the replacements is important. Check out provided files for further details and test cases. Parameters: Name Type Description Default text str a string required Returns: Type Description str normalized text Source code in klpt/preprocess.py def normalize ( self , text ): \"\"\" Text normalization This function deals with different encodings and unifies characters based on dialects and scripts as follows: - Sorani-Arabic: - replace frequent Arabic characters with their equivalent Kurdish ones, e.g. \"\u064a\" by \"\u06cc\" and \"\u0643\" by \"\u06a9\" - replace \"\u0647\" followed by zero-width non-joiner (ZWNJ, U+200C) with \"\u06d5\" where ZWNJ is removed (\"\u0631\u0647\u200c\u0632\u0628\u0647\u200c\u0631\" is converted to \"\u0631\u06d5\u0632\u0628\u06d5\u0631\"). ZWNJ in HTML is also taken into account. - replace \"\u0647\u0640\" with \"\u06be\" (U+06BE, ARABIC LETTER HEH DOACHASHMEE) - remove Kashida \"\u0640\" - \"\u06be\" in the middle of a word is replaced by \u0647 (U+0647) - replace different types of y, such as 'ARABIC LETTER ALEF MAKSURA' (U+0649) It should be noted that the order of the replacements is important. Check out provided files for further details and test cases. Arguments: text (str): a string Returns: str: normalized text \"\"\" temp_text = \" \" + self . unify_numerals ( text ) + \" \" for normalization_type in [ \"universal\" , self . dialect ]: for rep in self . preprocess_map [ \"normalizer\" ][ normalization_type ][ self . script ]: rep_tar = self . preprocess_map [ \"normalizer\" ][ normalization_type ][ self . script ][ rep ] temp_text = re . sub ( rf \" { rep } \" , rf \" { rep_tar } \" , temp_text , flags = re . I ) return temp_text . strip ()","title":"normalize()"},{"location":"user-guide/preprocess/#klpt.preprocess.Preprocess.preprocess","text":"One single function for normalization, standardization and unification of numerals Parameters: Name Type Description Default text str a string required Returns: Type Description str preprocessed text Source code in klpt/preprocess.py def preprocess ( self , text ): \"\"\" One single function for normalization, standardization and unification of numerals Arguments: text (str): a string Returns: str: preprocessed text \"\"\" return self . unify_numerals ( self . standardize ( self . normalize ( text )))","title":"preprocess()"},{"location":"user-guide/preprocess/#klpt.preprocess.Preprocess.standardize","text":"Method of standardization of Kurdish orthographies Given a normalized text, it returns standardized text based on the Kurdish orthographies. Sorani-Arabic: replace alveolar flap \u0631 (/\u027e/) at the begging of the word by the alveolar trill \u0695 (/r/) replace double rr and ll with \u0159 and \u0142 respectively Kurmanji-Latin: replace \"-an\" or \"'an\" in dates and numerals (\"di sala 2018'an\" and \"di sala 2018-an\" -> \"di sala 2018an\") Open issues: - replace \" \u0648\u06d5 \" by \" \u0648 \"? But this is not always possible, \"min bo we\" (\u0631\u06cc\u0632\u06af\u0640\u0631\u062a\u0646\u0627 \u0645\u0646 \u0628\u0648 \u0648\u06d5 \u0646\u06d5 \u0626\u06d5 \u0648\u06d5 \u0626\u0640\u0640\u06d5 \u0632) - \"pirt\u00fck\u00ea\": \"pirt\u00fbk\u00ea\"? - Should \u0131 (LATIN SMALL LETTER DOTLESS I be replaced by i? Parameters: Name Type Description Default text str a string required Returns: Type Description str standardized text Source code in klpt/preprocess.py def standardize ( self , text ): \"\"\" Method of standardization of Kurdish orthographies Given a normalized text, it returns standardized text based on the Kurdish orthographies. - Sorani-Arabic: - replace alveolar flap \u0631 (/\u027e/) at the begging of the word by the alveolar trill \u0695 (/r/) - replace double rr and ll with \u0159 and \u0142 respectively - Kurmanji-Latin: - replace \"-an\" or \"'an\" in dates and numerals (\"di sala 2018'an\" and \"di sala 2018-an\" -> \"di sala 2018an\") Open issues: - replace \" \u0648\u06d5 \" by \" \u0648 \"? But this is not always possible, \"min bo we\" (\u0631\u06cc\u0632\u06af\u0640\u0631\u062a\u0646\u0627 \u0645\u0646 \u0628\u0648 \u0648\u06d5 \u0646\u06d5 \u0626\u06d5 \u0648\u06d5 \u0626\u0640\u0640\u06d5 \u0632) - \"pirt\u00fck\u00ea\": \"pirt\u00fbk\u00ea\"? - Should [\u0131 (LATIN SMALL LETTER DOTLESS I](https://www.compart.com/en/unicode/U+0131) be replaced by i? Arguments: text (str): a string Returns: str: standardized text \"\"\" temp_text = \" \" + self . unify_numerals ( text ) + \" \" for standardization_type in [ self . dialect ]: for rep in self . preprocess_map [ \"standardizer\" ][ standardization_type ][ self . script ]: rep_tar = self . preprocess_map [ \"standardizer\" ][ standardization_type ][ self . script ][ rep ] temp_text = re . sub ( rf \" { rep } \" , rf \" { rep_tar } \" , temp_text , flags = re . I ) return temp_text . strip ()","title":"standardize()"},{"location":"user-guide/preprocess/#klpt.preprocess.Preprocess.unify_numerals","text":"Convert numerals to the desired one There are three types of numerals: - Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] - Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] - Latin [1234567890] Parameters: Name Type Description Default text str a string required Returns: Type Description str text with unified numerals Source code in klpt/preprocess.py def unify_numerals ( self , text ): \"\"\" Convert numerals to the desired one There are three types of numerals: - Arabic [\u0661\u0662\u0663\u0664\u0665\u0666\u0667\u0668\u0669\u0660] - Farsi [\u06f1\u06f2\u06f3\u06f4\u06f5\u06f6\u06f7\u06f8\u06f9\u06f0] - Latin [1234567890] Arguments: text (str): a string Returns: str: text with unified numerals \"\"\" for i , j in self . preprocess_map [ \"normalizer\" ][ \"universal\" ][ \"numerals\" ][ self . numeral ] . items (): text = text . replace ( i , j ) return text","title":"unify_numerals()"},{"location":"user-guide/stem/","text":"stem package The Stem module deals with various tasks, mainly through the following functions: - check_spelling : spell error detection - correct_spelling : spell error correction - analyze : morphological analysis - stem : stemming, e.g. \"\u0628\u0695\u0627\u0648\u06d5\" \u2192 \"\u0628\u0695\" - lemmatize : lemmatization, e.g. \"\u0628\u0631\u062f\u0645\u0646\u06d5\u0648\u06d5\" \u2192 \"\u0628\u0631\u062f\u0646\" It is recommended that this module be used on tokens using the tokenization module. Please note that only Sorani is supported in this version in this module. The module is based on the Kurdish Hunspell project . Regarding stemming, the following procedure is followed: - for tokens of a single word, as \"kirin\" (to do), the stem of the token is returned. - for compound forms and multi-word expressions, the stem of the noun, adjective or adverb are taken into account. For instance, in the light verbal constructions such as \"bar kirin\" (to load), the stem of the nominal component \"bar\" is returned. In other cases, the stem of that part of the MWE token is returned that is semantically more important, as in \"\u062f\u06d5\u0633\u062a \u062a\u06ce \u0648\u06d5\u0631\u062f\u0627\u0646\" (dest-t\u00ea-werdan) where the stem of \"dest\" is returned. Examples: >>> from klpt.stem import Stem >>> stemmer = Stem(\"Sorani\", \"Arabic\") >>> stemmer.check_spelling(\"\u0633\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a\") False >>> stemmer.correct_spelling(\"\u0633\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a\") (False, ['\u0633\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0633\u0648\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0633\u0648\u0648\u0695\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0695\u0648\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0641\u06d5\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0628\u0648\u0648\u0698\u0627\u0646\u062f\u0628\u0648\u0648\u062a']) >>> stemmer.analyze(\"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\") [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] >>> stemmer.stem(\"\u062f\u06d5\u0686\u06cc\u0646\u06d5\u0648\u06d5\") ['\u0686'] >>> stemmer.stem(\"\u06af\u0648\u0631\u06d5\u06a9\u06d5\", mark_unknown=True) ['_\u06af\u0648\u0631_'] >>> stemmer.lemmatize(\"\u06af\u0648\u06b5\u06d5\u06a9\u0627\u0646\u0645\") ['\u06af\u0648\u06b5', '\u06af\u0648\u06b5\u06d5'] >>> stemmer = Stem(\"Kurmanji\", \"Latin\") >>> stemmer.analyze(\"dib\u00eajim\") [{'base': 'gotin', 'description': 'vblex_tv_pri_p1_sg', 'pos': '', 'terminal_suffix': '', 'formation': ''}] analyze ( self , word_form ) Morphological analysis of a given word. It returns morphological analyses. The morphological analysis is returned as a dictionary as follows: \"pos\": the part-of-speech of the word-form according to the Universal Dependency tag set . \"description\": is flag \"prefixes\": anything appearing before the base \"suffixes\": anything appearing after the base \"st\": the stem of the word \"lem\": the lemma of the word \"formation\": if ds flag is set, its value is assigned to description and the value of formation is set to derivational. Although the majority of our morphological rules cover inflectional forms, it is not accurate to say all of them are inflectional. Therefore, we only set this value to derivational wherever we are sure. \"base\": ts flag. The definition of terminal suffix is a bit tricky in Hunspell. According to the Hunspell documentation , \"Terminal suffix fields are inflectional suffix fields \"removed\" by additional (not terminal) suffixes\". In other words, the ts flag in Hunspell represents whatever is left after stripping all affixes. Therefore, it is the morphological base. As for the word \"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\" (that I have seen them), the morphological analysis would look like this: [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] If the input cannot be analyzed morphologically, an empty list is returned. Sorani: More details regarding Sorani Kurdish morphological analysis can be found at https://github.com/sinaahmadi/KurdishHunspell . Kurmanji: Regarding Kurmanji, we use the morphological analyzer provided by the Kurmanji part Please note that there are delicate difference between who the analyzers work in Hunspell and Apertium. For instane, the base in the Kurmanji analysis refers to the lemma while in Sorani (from Hunspell), it refers to the morphological base. Parameters: Name Type Description Default word_form str a single word-form required Exceptions: Type Description TypeError only string as input Returns: Type Description (list(dict)) a list of all possible morphological analyses according to the defined morphological rules Source code in klpt/stem.py def analyze ( self , word_form ): \"\"\" Morphological analysis of a given word. It returns morphological analyses. The morphological analysis is returned as a dictionary as follows: - \"pos\": the part-of-speech of the word-form according to [the Universal Dependency tag set](https://universaldependencies.org/u/pos/index.html). - \"description\": is flag - \"prefixes\": anything appearing before the base - \"suffixes\": anything appearing after the base - \"st\": the stem of the word - \"lem\": the lemma of the word - \"formation\": if ds flag is set, its value is assigned to description and the value of formation is set to derivational. Although the majority of our morphological rules cover inflectional forms, it is not accurate to say all of them are inflectional. Therefore, we only set this value to derivational wherever we are sure. - \"base\": `ts` flag. The definition of terminal suffix is a bit tricky in Hunspell. According to [the Hunspell documentation](http://manpages.ubuntu.com/manpages/trusty/en/man4/hunspell.4.html), \"Terminal suffix fields are inflectional suffix fields \"removed\" by additional (not terminal) suffixes\". In other words, the ts flag in Hunspell represents whatever is left after stripping all affixes. Therefore, it is the morphological base. As for the word \"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\" (that I have seen them), the morphological analysis would look like this: [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] If the input cannot be analyzed morphologically, an empty list is returned. Sorani: More details regarding Sorani Kurdish morphological analysis can be found at [https://github.com/sinaahmadi/KurdishHunspell](https://github.com/sinaahmadi/KurdishHunspell). Kurmanji: Regarding Kurmanji, we use the morphological analyzer provided by the [Kurmanji part](https://github.com/apertium/apertium-kmr) Please note that there are delicate difference between who the analyzers work in Hunspell and Apertium. For instane, the `base` in the Kurmanji analysis refers to the lemma while in Sorani (from Hunspell), it refers to the morphological base. Args: word_form (str): a single word-form Raises: TypeError: only string as input Returns: (list(dict)): a list of all possible morphological analyses according to the defined morphological rules \"\"\" if not isinstance ( word_form , str ): raise TypeError ( \"Only a word (str) is allowed.\" ) else : word_analysis = list () if self . dialect == \"Sorani\" and self . script == \"Arabic\" : # Given the morphological analysis of a word-form with Hunspell flags, extract relevant information and return a dictionary # print(self.huns.analyze(word_form)) for analysis in list ( self . huns . analyze ( word_form )): analysis_dict = dict () for item in analysis . split (): if \":\" not in item : continue if item . split ( \":\" )[ 1 ] == \"ts\" : # ts flag exceptionally appears after the value as value:key in the Hunspell output # anything except the terminal_suffix (ts) is considered to be the base analysis_dict [ \"base\" ] = item . split ( \":\" )[ 0 ] affixes = utility . extract_prefix_suffix ( word_form , item . split ( \":\" )[ 0 ]) analysis_dict [ \"prefixes\" ] = affixes [ 0 ] analysis_dict [ \"suffixes\" ] = affixes [ 2 ] elif item . split ( \":\" )[ 0 ] in self . hunspell_flags . keys (): # assign the key:value pairs from the Hunspell string output to the dictionary output of the current function if item . split ( \":\" )[ 0 ] == \"ds\" : # for ds flag, add derivation as the formation type, otherwise inflection analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = \"derivational\" analysis_dict [ self . hunspell_flags [ \"is\" ]] = item . split ( \":\" )[ 1 ] elif item . split ( \":\" )[ 0 ] == \"st\" : # for st flag, stem should be cleaned first analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = self . clean_stem ( item . split ( \":\" )[ 1 ]) else : # remove I, T or V using clean_stem() analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = self . clean_stem ( item . split ( \":\" )[ 1 ]) # convert lemma and pos to a list and split based on _ when there is more than one output, e.g. more than one lemma for a given word if \"lemma\" in analysis_dict : analysis_dict [ \"lemma\" ] = analysis_dict [ \"lemma\" ] . split ( \"_\" ) else : analysis_dict [ \"lemma\" ] = [ \"\" ] if \"pos\" in analysis_dict : analysis_dict [ \"pos\" ] = analysis_dict [ \"pos\" ] . split ( \"_\" ) else : analysis_dict [ \"pos\" ] = [ \"\" ] # for nouns, base is lemma if len ( analysis_dict [ \"pos\" ]) and analysis_dict [ \"pos\" ] != [ \"verb\" ]: analysis_dict [ \"lemma\" ] = [ analysis_dict [ \"base\" ]] word_analysis . append ( analysis_dict ) elif self . dialect == \"Kurmanji\" and self . script == \"Latin\" : att_analysis = Analysis ( \"Kurmanji\" , \"Latin\" ) . analyze ( word_form ) # check if the word-form is analyzed or no if not len ( att_analysis ): # the word-form could not be analyzed return [] for analysis in att_analysis : analysis_dict = dict () structure = analysis [ 0 ] . split ( \"<\" , 1 ) analysis_dict [ \"base\" ], analysis_dict [ \"description\" ] = structure [ 0 ], structure [ 1 ] . replace ( \"><\" , \"_\" ) . replace ( \">\" , \"\" ) . strip () analysis_dict [ \"pos\" ] = \"\" analysis_dict [ \"terminal_suffix\" ] = \"\" analysis_dict [ \"formation\" ] = \"\" # TODO: the description needs further information extraction in such a way that some values should be assigned to the \"pos\" key # analysis_dict[\"terminal_suffix\"] = word_form.replace(analysis_dict[\"base\"], \"\") word_analysis . append ( analysis_dict ) return word_analysis check_spelling ( self , word ) Check spelling of a word Parameters: Name Type Description Default word str input word to be spell-checked required Exceptions: Type Description TypeError only string as input Returns: Type Description bool True if the spelling is correct, False if the spelling is incorrect Source code in klpt/stem.py def check_spelling ( self , word ): \"\"\"Check spelling of a word Args: word (str): input word to be spell-checked Raises: TypeError: only string as input Returns: bool: True if the spelling is correct, False if the spelling is incorrect \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : return self . huns . spell ( word ) clean_stem ( self , word ) Remove extra characters in the stem The following issue was observed when stemming with Hunspell (version 2.0.2) where the retrieved stem of a verb is accompanied by the flag of the word, which is an unwanted extra character. Possible flags are T, V and I. :lf should also be taken into account. Parameters: Name Type Description Default word [str] [stem] required Source code in klpt/stem.py def clean_stem ( self , word ): \"\"\"Remove extra characters in the stem The following issue was observed when stemming with Hunspell (version 2.0.2) where the retrieved stem of a verb is accompanied by the flag of the word, which is an unwanted extra character. Possible flags are T, V and I. :lf should also be taken into account. Args: word ([str]): [stem] \"\"\" for char in [ \"V\" , \"I\" , \"T\" ]: word = word . replace ( char , \"\" ) return word . replace ( \":lf\" , \"\" ) correct_spelling ( self , word ) Correct spelling errors if the input word is incorrect. It returns a tuple where the first element indicates the correctness of the word (True if correct, False if incorrect). If the input word is incorrect, suggestions are provided in a list as the second element of the tuple, as (False, []). If no suggestion is available, the list is returned empty as (True, []). Parameters: Name Type Description Default word str input word to be spell-checked required Exceptions: Type Description TypeError only string as input Returns: Type Description tuple (boolean, list) Source code in klpt/stem.py def correct_spelling ( self , word ): \"\"\" Correct spelling errors if the input word is incorrect. It returns a tuple where the first element indicates the correctness of the word (True if correct, False if incorrect). If the input word is incorrect, suggestions are provided in a list as the second element of the tuple, as (False, []). If no suggestion is available, the list is returned empty as (True, []). Args: word (str): input word to be spell-checked Raises: TypeError: only string as input Returns: tuple (boolean, list) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : if self . check_spelling ( word ): return ( True , []) return ( False , list ( self . huns . suggest ( word ))) lemmatize ( self , word ) A function for lemmatization of words Parameters: Name Type Description Default word [str] [given a word, return its lemma form, i.e. dictionary entry form] required Exceptions: Type Description TypeError only string as input Returns: Type Description list list of lemma(s) Source code in klpt/stem.py def lemmatize ( self , word ): \"\"\"A function for lemmatization of words Args: word ([str]): [given a word, return its lemma form, i.e. dictionary entry form] Raises: TypeError: only string as input Returns: list: list of lemma(s) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : word_analysis = self . analyze ( word ) return list ( set ([ item for sublist in word_analysis for item in sublist [ \"lemma\" ] if item != '' ])) stem ( self , word , mark_unknown = False ) A function for stemming a single word Parameters: Name Type Description Default word str input word to be spell-checked required mark_unknown False if the given word is unknown in the tagged lexicon, KLPT stems is following rules. Such stems can be marked with \"_\" if this variable set to True False Exceptions: Type Description TypeError only string as input Returns: Type Description list list of stem(s) Source code in klpt/stem.py def stem ( self , word , mark_unknown = False ): \"\"\"A function for stemming a single word Args: word (str): input word to be spell-checked mark_unknown (False): if the given word is unknown in the tagged lexicon, KLPT stems is following rules. Such stems can be marked with \"_\" if this variable set to True Raises: TypeError: only string as input Returns: list: list of stem(s) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word )])) if len ( stems ): return stems else : # not detected by Hunspell or the word doesn't exist in the tagged lexicon for verb in self . light_verbs : if word . endswith ( verb ) and len ( word . rpartition ( verb )[ 0 ]): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( verb )[ 0 ] . strip ())])) if len ( stems ): # the word is a compound form with a light verb. The other part can be stemmed by Hunspell return stems else : # the word is a compound form with a light verb but the other part cannot be stemmed by Hunspell word = word . rpartition ( verb )[ 0 ] . strip () # the other part of the word or the whole word cannot be stemmed by Hunspell # so, find the stem following morphological rules by checking if removing possible prefixes and suffixes would help finding the stem # Note: even though the same morphemes used in the tokenization system are used in the rules here, there is a delicate difference. # In the tokenization system, the trimming is done in such a way that shorter morphemes are first checked for suffixes (suffixes in the json file is sorted by length) and # longer prefixes are trimmer first. # For the stemmer, however, we do differently by first checking the longer morphemes then shorter ones (for both prefixes and suffixes). # This is due to the different purposes of the two tasks. Therefore, the list of the morphemes is to be reversed for suffixes (not prefixes). for preposition in self . morphemes [ \"prefixes\" ]: if word . startswith ( preposition ) and len ( word . split ( preposition , 1 )) > 1 : if len ( list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . split ( preposition , 1 )[ 1 ])]))): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . split ( preposition , 1 )[ 1 ])])) if mark_unknown : return [ \"_\" + i + \"_\" for i in stems ] else : word = word . split ( preposition , 1 )[ 1 ] break for postposition in reversed ( list ( self . morphemes [ \"suffixes\" ])): if word . endswith ( postposition ) and len ( word . rpartition ( postposition )[ 0 ]): if len ( list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( postposition )[ 0 ])]))): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( postposition )[ 0 ])])) if mark_unknown : return [ \"_\" + i + \"_\" for i in stems ] else : word = word . rpartition ( postposition )[ 0 ] break # not possible to stem the word using the tagged lexicon or the rule-based approach. Return the word as it is. if mark_unknown : return [ \"_\" + word + \"_\" ] else : return [ word ]","title":"Stem"},{"location":"user-guide/stem/#stem-package","text":"The Stem module deals with various tasks, mainly through the following functions: - check_spelling : spell error detection - correct_spelling : spell error correction - analyze : morphological analysis - stem : stemming, e.g. \"\u0628\u0695\u0627\u0648\u06d5\" \u2192 \"\u0628\u0695\" - lemmatize : lemmatization, e.g. \"\u0628\u0631\u062f\u0645\u0646\u06d5\u0648\u06d5\" \u2192 \"\u0628\u0631\u062f\u0646\" It is recommended that this module be used on tokens using the tokenization module. Please note that only Sorani is supported in this version in this module. The module is based on the Kurdish Hunspell project . Regarding stemming, the following procedure is followed: - for tokens of a single word, as \"kirin\" (to do), the stem of the token is returned. - for compound forms and multi-word expressions, the stem of the noun, adjective or adverb are taken into account. For instance, in the light verbal constructions such as \"bar kirin\" (to load), the stem of the nominal component \"bar\" is returned. In other cases, the stem of that part of the MWE token is returned that is semantically more important, as in \"\u062f\u06d5\u0633\u062a \u062a\u06ce \u0648\u06d5\u0631\u062f\u0627\u0646\" (dest-t\u00ea-werdan) where the stem of \"dest\" is returned. Examples: >>> from klpt.stem import Stem >>> stemmer = Stem(\"Sorani\", \"Arabic\") >>> stemmer.check_spelling(\"\u0633\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a\") False >>> stemmer.correct_spelling(\"\u0633\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a\") (False, ['\u0633\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0633\u0648\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0633\u0648\u0648\u0695\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0695\u0648\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0641\u06d5\u0648\u062a\u0627\u0646\u062f\u0628\u0648\u0648\u062a', '\u0628\u0648\u0648\u0698\u0627\u0646\u062f\u0628\u0648\u0648\u062a']) >>> stemmer.analyze(\"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\") [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] >>> stemmer.stem(\"\u062f\u06d5\u0686\u06cc\u0646\u06d5\u0648\u06d5\") ['\u0686'] >>> stemmer.stem(\"\u06af\u0648\u0631\u06d5\u06a9\u06d5\", mark_unknown=True) ['_\u06af\u0648\u0631_'] >>> stemmer.lemmatize(\"\u06af\u0648\u06b5\u06d5\u06a9\u0627\u0646\u0645\") ['\u06af\u0648\u06b5', '\u06af\u0648\u06b5\u06d5'] >>> stemmer = Stem(\"Kurmanji\", \"Latin\") >>> stemmer.analyze(\"dib\u00eajim\") [{'base': 'gotin', 'description': 'vblex_tv_pri_p1_sg', 'pos': '', 'terminal_suffix': '', 'formation': ''}]","title":"stem package"},{"location":"user-guide/stem/#klpt.stem.Stem.analyze","text":"Morphological analysis of a given word. It returns morphological analyses. The morphological analysis is returned as a dictionary as follows: \"pos\": the part-of-speech of the word-form according to the Universal Dependency tag set . \"description\": is flag \"prefixes\": anything appearing before the base \"suffixes\": anything appearing after the base \"st\": the stem of the word \"lem\": the lemma of the word \"formation\": if ds flag is set, its value is assigned to description and the value of formation is set to derivational. Although the majority of our morphological rules cover inflectional forms, it is not accurate to say all of them are inflectional. Therefore, we only set this value to derivational wherever we are sure. \"base\": ts flag. The definition of terminal suffix is a bit tricky in Hunspell. According to the Hunspell documentation , \"Terminal suffix fields are inflectional suffix fields \"removed\" by additional (not terminal) suffixes\". In other words, the ts flag in Hunspell represents whatever is left after stripping all affixes. Therefore, it is the morphological base. As for the word \"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\" (that I have seen them), the morphological analysis would look like this: [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] If the input cannot be analyzed morphologically, an empty list is returned. Sorani: More details regarding Sorani Kurdish morphological analysis can be found at https://github.com/sinaahmadi/KurdishHunspell . Kurmanji: Regarding Kurmanji, we use the morphological analyzer provided by the Kurmanji part Please note that there are delicate difference between who the analyzers work in Hunspell and Apertium. For instane, the base in the Kurmanji analysis refers to the lemma while in Sorani (from Hunspell), it refers to the morphological base. Parameters: Name Type Description Default word_form str a single word-form required Exceptions: Type Description TypeError only string as input Returns: Type Description (list(dict)) a list of all possible morphological analyses according to the defined morphological rules Source code in klpt/stem.py def analyze ( self , word_form ): \"\"\" Morphological analysis of a given word. It returns morphological analyses. The morphological analysis is returned as a dictionary as follows: - \"pos\": the part-of-speech of the word-form according to [the Universal Dependency tag set](https://universaldependencies.org/u/pos/index.html). - \"description\": is flag - \"prefixes\": anything appearing before the base - \"suffixes\": anything appearing after the base - \"st\": the stem of the word - \"lem\": the lemma of the word - \"formation\": if ds flag is set, its value is assigned to description and the value of formation is set to derivational. Although the majority of our morphological rules cover inflectional forms, it is not accurate to say all of them are inflectional. Therefore, we only set this value to derivational wherever we are sure. - \"base\": `ts` flag. The definition of terminal suffix is a bit tricky in Hunspell. According to [the Hunspell documentation](http://manpages.ubuntu.com/manpages/trusty/en/man4/hunspell.4.html), \"Terminal suffix fields are inflectional suffix fields \"removed\" by additional (not terminal) suffixes\". In other words, the ts flag in Hunspell represents whatever is left after stripping all affixes. Therefore, it is the morphological base. As for the word \"\u062f\u06cc\u062a\u0628\u0627\u0645\u0646\" (that I have seen them), the morphological analysis would look like this: [{'pos': ['verb'], 'description': 'past_stem_transitive_active', 'stem': '\u062f\u06cc', 'lemma': ['\u062f\u06cc\u062a\u0646'], 'base': '\u062f\u06cc\u062a', 'prefixes': '', 'suffixes': '\u0628\u0627\u0645\u0646'}] If the input cannot be analyzed morphologically, an empty list is returned. Sorani: More details regarding Sorani Kurdish morphological analysis can be found at [https://github.com/sinaahmadi/KurdishHunspell](https://github.com/sinaahmadi/KurdishHunspell). Kurmanji: Regarding Kurmanji, we use the morphological analyzer provided by the [Kurmanji part](https://github.com/apertium/apertium-kmr) Please note that there are delicate difference between who the analyzers work in Hunspell and Apertium. For instane, the `base` in the Kurmanji analysis refers to the lemma while in Sorani (from Hunspell), it refers to the morphological base. Args: word_form (str): a single word-form Raises: TypeError: only string as input Returns: (list(dict)): a list of all possible morphological analyses according to the defined morphological rules \"\"\" if not isinstance ( word_form , str ): raise TypeError ( \"Only a word (str) is allowed.\" ) else : word_analysis = list () if self . dialect == \"Sorani\" and self . script == \"Arabic\" : # Given the morphological analysis of a word-form with Hunspell flags, extract relevant information and return a dictionary # print(self.huns.analyze(word_form)) for analysis in list ( self . huns . analyze ( word_form )): analysis_dict = dict () for item in analysis . split (): if \":\" not in item : continue if item . split ( \":\" )[ 1 ] == \"ts\" : # ts flag exceptionally appears after the value as value:key in the Hunspell output # anything except the terminal_suffix (ts) is considered to be the base analysis_dict [ \"base\" ] = item . split ( \":\" )[ 0 ] affixes = utility . extract_prefix_suffix ( word_form , item . split ( \":\" )[ 0 ]) analysis_dict [ \"prefixes\" ] = affixes [ 0 ] analysis_dict [ \"suffixes\" ] = affixes [ 2 ] elif item . split ( \":\" )[ 0 ] in self . hunspell_flags . keys (): # assign the key:value pairs from the Hunspell string output to the dictionary output of the current function if item . split ( \":\" )[ 0 ] == \"ds\" : # for ds flag, add derivation as the formation type, otherwise inflection analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = \"derivational\" analysis_dict [ self . hunspell_flags [ \"is\" ]] = item . split ( \":\" )[ 1 ] elif item . split ( \":\" )[ 0 ] == \"st\" : # for st flag, stem should be cleaned first analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = self . clean_stem ( item . split ( \":\" )[ 1 ]) else : # remove I, T or V using clean_stem() analysis_dict [ self . hunspell_flags [ item . split ( \":\" )[ 0 ]]] = self . clean_stem ( item . split ( \":\" )[ 1 ]) # convert lemma and pos to a list and split based on _ when there is more than one output, e.g. more than one lemma for a given word if \"lemma\" in analysis_dict : analysis_dict [ \"lemma\" ] = analysis_dict [ \"lemma\" ] . split ( \"_\" ) else : analysis_dict [ \"lemma\" ] = [ \"\" ] if \"pos\" in analysis_dict : analysis_dict [ \"pos\" ] = analysis_dict [ \"pos\" ] . split ( \"_\" ) else : analysis_dict [ \"pos\" ] = [ \"\" ] # for nouns, base is lemma if len ( analysis_dict [ \"pos\" ]) and analysis_dict [ \"pos\" ] != [ \"verb\" ]: analysis_dict [ \"lemma\" ] = [ analysis_dict [ \"base\" ]] word_analysis . append ( analysis_dict ) elif self . dialect == \"Kurmanji\" and self . script == \"Latin\" : att_analysis = Analysis ( \"Kurmanji\" , \"Latin\" ) . analyze ( word_form ) # check if the word-form is analyzed or no if not len ( att_analysis ): # the word-form could not be analyzed return [] for analysis in att_analysis : analysis_dict = dict () structure = analysis [ 0 ] . split ( \"<\" , 1 ) analysis_dict [ \"base\" ], analysis_dict [ \"description\" ] = structure [ 0 ], structure [ 1 ] . replace ( \"><\" , \"_\" ) . replace ( \">\" , \"\" ) . strip () analysis_dict [ \"pos\" ] = \"\" analysis_dict [ \"terminal_suffix\" ] = \"\" analysis_dict [ \"formation\" ] = \"\" # TODO: the description needs further information extraction in such a way that some values should be assigned to the \"pos\" key # analysis_dict[\"terminal_suffix\"] = word_form.replace(analysis_dict[\"base\"], \"\") word_analysis . append ( analysis_dict ) return word_analysis","title":"analyze()"},{"location":"user-guide/stem/#klpt.stem.Stem.check_spelling","text":"Check spelling of a word Parameters: Name Type Description Default word str input word to be spell-checked required Exceptions: Type Description TypeError only string as input Returns: Type Description bool True if the spelling is correct, False if the spelling is incorrect Source code in klpt/stem.py def check_spelling ( self , word ): \"\"\"Check spelling of a word Args: word (str): input word to be spell-checked Raises: TypeError: only string as input Returns: bool: True if the spelling is correct, False if the spelling is incorrect \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : return self . huns . spell ( word )","title":"check_spelling()"},{"location":"user-guide/stem/#klpt.stem.Stem.clean_stem","text":"Remove extra characters in the stem The following issue was observed when stemming with Hunspell (version 2.0.2) where the retrieved stem of a verb is accompanied by the flag of the word, which is an unwanted extra character. Possible flags are T, V and I. :lf should also be taken into account. Parameters: Name Type Description Default word [str] [stem] required Source code in klpt/stem.py def clean_stem ( self , word ): \"\"\"Remove extra characters in the stem The following issue was observed when stemming with Hunspell (version 2.0.2) where the retrieved stem of a verb is accompanied by the flag of the word, which is an unwanted extra character. Possible flags are T, V and I. :lf should also be taken into account. Args: word ([str]): [stem] \"\"\" for char in [ \"V\" , \"I\" , \"T\" ]: word = word . replace ( char , \"\" ) return word . replace ( \":lf\" , \"\" )","title":"clean_stem()"},{"location":"user-guide/stem/#klpt.stem.Stem.correct_spelling","text":"Correct spelling errors if the input word is incorrect. It returns a tuple where the first element indicates the correctness of the word (True if correct, False if incorrect). If the input word is incorrect, suggestions are provided in a list as the second element of the tuple, as (False, []). If no suggestion is available, the list is returned empty as (True, []). Parameters: Name Type Description Default word str input word to be spell-checked required Exceptions: Type Description TypeError only string as input Returns: Type Description tuple (boolean, list) Source code in klpt/stem.py def correct_spelling ( self , word ): \"\"\" Correct spelling errors if the input word is incorrect. It returns a tuple where the first element indicates the correctness of the word (True if correct, False if incorrect). If the input word is incorrect, suggestions are provided in a list as the second element of the tuple, as (False, []). If no suggestion is available, the list is returned empty as (True, []). Args: word (str): input word to be spell-checked Raises: TypeError: only string as input Returns: tuple (boolean, list) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : if self . check_spelling ( word ): return ( True , []) return ( False , list ( self . huns . suggest ( word )))","title":"correct_spelling()"},{"location":"user-guide/stem/#klpt.stem.Stem.lemmatize","text":"A function for lemmatization of words Parameters: Name Type Description Default word [str] [given a word, return its lemma form, i.e. dictionary entry form] required Exceptions: Type Description TypeError only string as input Returns: Type Description list list of lemma(s) Source code in klpt/stem.py def lemmatize ( self , word ): \"\"\"A function for lemmatization of words Args: word ([str]): [given a word, return its lemma form, i.e. dictionary entry form] Raises: TypeError: only string as input Returns: list: list of lemma(s) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : word_analysis = self . analyze ( word ) return list ( set ([ item for sublist in word_analysis for item in sublist [ \"lemma\" ] if item != '' ]))","title":"lemmatize()"},{"location":"user-guide/stem/#klpt.stem.Stem.stem","text":"A function for stemming a single word Parameters: Name Type Description Default word str input word to be spell-checked required mark_unknown False if the given word is unknown in the tagged lexicon, KLPT stems is following rules. Such stems can be marked with \"_\" if this variable set to True False Exceptions: Type Description TypeError only string as input Returns: Type Description list list of stem(s) Source code in klpt/stem.py def stem ( self , word , mark_unknown = False ): \"\"\"A function for stemming a single word Args: word (str): input word to be spell-checked mark_unknown (False): if the given word is unknown in the tagged lexicon, KLPT stems is following rules. Such stems can be marked with \"_\" if this variable set to True Raises: TypeError: only string as input Returns: list: list of stem(s) \"\"\" if not isinstance ( word , str ) or not ( self . dialect == \"Sorani\" and self . script == \"Arabic\" ): raise TypeError ( \"Not supported yet.\" ) else : stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word )])) if len ( stems ): return stems else : # not detected by Hunspell or the word doesn't exist in the tagged lexicon for verb in self . light_verbs : if word . endswith ( verb ) and len ( word . rpartition ( verb )[ 0 ]): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( verb )[ 0 ] . strip ())])) if len ( stems ): # the word is a compound form with a light verb. The other part can be stemmed by Hunspell return stems else : # the word is a compound form with a light verb but the other part cannot be stemmed by Hunspell word = word . rpartition ( verb )[ 0 ] . strip () # the other part of the word or the whole word cannot be stemmed by Hunspell # so, find the stem following morphological rules by checking if removing possible prefixes and suffixes would help finding the stem # Note: even though the same morphemes used in the tokenization system are used in the rules here, there is a delicate difference. # In the tokenization system, the trimming is done in such a way that shorter morphemes are first checked for suffixes (suffixes in the json file is sorted by length) and # longer prefixes are trimmer first. # For the stemmer, however, we do differently by first checking the longer morphemes then shorter ones (for both prefixes and suffixes). # This is due to the different purposes of the two tasks. Therefore, the list of the morphemes is to be reversed for suffixes (not prefixes). for preposition in self . morphemes [ \"prefixes\" ]: if word . startswith ( preposition ) and len ( word . split ( preposition , 1 )) > 1 : if len ( list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . split ( preposition , 1 )[ 1 ])]))): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . split ( preposition , 1 )[ 1 ])])) if mark_unknown : return [ \"_\" + i + \"_\" for i in stems ] else : word = word . split ( preposition , 1 )[ 1 ] break for postposition in reversed ( list ( self . morphemes [ \"suffixes\" ])): if word . endswith ( postposition ) and len ( word . rpartition ( postposition )[ 0 ]): if len ( list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( postposition )[ 0 ])]))): stems = list ( set ([ self . clean_stem ( i ) for i in self . huns . stem ( word . rpartition ( postposition )[ 0 ])])) if mark_unknown : return [ \"_\" + i + \"_\" for i in stems ] else : word = word . rpartition ( postposition )[ 0 ] break # not possible to stem the word using the tagged lexicon or the rule-based approach. Return the word as it is. if mark_unknown : return [ \"_\" + word + \"_\" ] else : return [ word ]","title":"stem()"},{"location":"user-guide/tokenize/","text":"tokenize package This module focuses on the tokenization of both Kurmanji and Sorani dialects of Kurdish with the following functions: word_tokenize : tokenization of texts into tokens (both multi-word expressions and single-word tokens). mwe_tokenize : tokenization of texts by only taking compound forms into account sent_tokenize : tokenization of texts into sentences The module is based on the Kurdish tokenization project . Examples: >>> from klpt.tokenize import Tokenize >>> tokenizer = Tokenize(\"Kurmanji\", \"Latin\") >>> tokenizer.word_tokenize(\"ji bo fort\u00ea xwe av\u00eatin\") ['\u2581ji\u2581', 'bo', '\u2581\u2581fort\u00ea\u2012xwe\u2012av\u00eatin\u2581\u2581'] >>> tokenizer.mwe_tokenize(\"bi serok\u00ea huk\u00fbmeta her\u00eama Kurdistan\u00ea Prof. Salih re saz kir.\") 'bi serok\u00ea huk\u00fbmeta her\u00eama Kurdistan\u00ea Prof . Salih re saz kir .' >>> tokenizer_ckb = Tokenize(\"Sorani\", \"Arabic\") >>> tokenizer_ckb.word_tokenize(\"\u0628\u06d5 \u0647\u06d5\u0645\u0648\u0648 \u0647\u06d5\u0645\u0648\u0648\u0627\u0646\u06d5\u0648\u06d5 \u0695\u06ce\u06a9 \u06a9\u06d5\u0648\u062a\u0646\") ['\u2581\u0628\u06d5\u2581', '\u2581\u0647\u06d5\u0645\u0648\u0648\u2581', '\u0647\u06d5\u0645\u0648\u0648\u0627\u0646\u06d5\u0648\u06d5', '\u2581\u2581\u0695\u06ce\u06a9\u2012\u06a9\u06d5\u0648\u062a\u0646\u2581\u2581'] mwe_tokenize ( self , sentence , separator = '\u2581\u2581' , in_separator = '\u2012' , punct_marked = False , keep_form = False ) Multi-word expression tokenization Parameters: Name Type Description Default sentence str sentence to be split by multi-word expressions required separator str a specific token to specify a multi-word expression. By default two \u2581 (\u2581\u2581) are used for this purpose. '\u2581\u2581' in_separator str a specific token to specify the composing parts of a multi-word expression. By default a dash - is used for this purpose. '\u2012' keep_form boolean if set to True, the original form of the multi-word expression is returned the same way provided in the input. On the other hand, if set to False, the lemma form is used where the parts are delimited by a dash \u2012, as in \"dab\u2012\u00fb\u2012ner\u00eet\" False Returns: Type Description str sentence containing d multi-word expressions using the separator Source code in klpt/tokenize.py def mwe_tokenize ( self , sentence , separator = \"\u2581\u2581\" , in_separator = \"\u2012\" , punct_marked = False , keep_form = False ): \"\"\" Multi-word expression tokenization Args: sentence (str): sentence to be split by multi-word expressions separator (str): a specific token to specify a multi-word expression. By default two \u2581 (\u2581\u2581) are used for this purpose. in_separator (str): a specific token to specify the composing parts of a multi-word expression. By default a dash - is used for this purpose. keep_form (boolean): if set to True, the original form of the multi-word expression is returned the same way provided in the input. On the other hand, if set to False, the lemma form is used where the parts are delimited by a dash \u2012, as in \"dab\u2012\u00fb\u2012ner\u00eet\" Returns: str: sentence containing d multi-word expressions using the separator \"\"\" sentence = \" \" + sentence + \" \" if not punct_marked : # find punctuation marks and add a space around for punct in self . tokenize_map [ \"word_tokenize\" ][ self . dialect ][ self . script ][ \"punctuation\" ]: if punct in sentence : sentence = sentence . replace ( punct , \" \" + punct + \" \" ) # look for compound words and delimit them by double separator for compound_lemma in self . mwe_lexicon : compound_lemma_context = \" \" + compound_lemma + \" \" if compound_lemma_context in sentence : if keep_form : sentence = sentence . replace ( compound_lemma_context , \" \u2581\u2581\" + compound_lemma + \"\u2581\u2581 \" ) else : sentence = sentence . replace ( compound_lemma_context , \" \u2581\u2581\" + compound_lemma . replace ( \"-\" , in_separator ) + \"\u2581\u2581 \" ) # check the possible word forms available for each compound lemma in the lex files, too # Note: compound forms don't have any hyphen or separator in the lex files for compound_form in self . mwe_lexicon [ compound_lemma ][ \"token_forms\" ]: compound_form_context = \" \" + compound_form + \" \" if compound_form_context in sentence : if keep_form : sentence = sentence . replace ( compound_form_context , \" \u2581\u2581\" + compound_form + \"\u2581\u2581 \" ) else : sentence = sentence . replace ( compound_form_context , \" \u2581\u2581\" + compound_lemma . replace ( \"-\" , in_separator ) + \"\u2581\u2581 \" ) # print(sentence) return sentence . replace ( \" \" , \" \" ) . replace ( \"\u2581\u2581\" , separator ) . strip () sent_tokenize ( self , text ) Sentence tokenizer Parameters: Name Type Description Default text [str] [input text to be tokenized by sentences] required Returns: Type Description [list] [a list of sentences] Source code in klpt/tokenize.py def sent_tokenize ( self , text ): \"\"\"Sentence tokenizer Args: text ([str]): [input text to be tokenized by sentences] Returns: [list]: [a list of sentences] \"\"\" text = \" \" + text + \" \" text = text . replace ( \" \\n \" , \" \" ) text = re . sub ( self . prefixes , \" \\\\ 1<prd>\" , text ) text = re . sub ( self . websites , \"<prd> \\\\ 1\" , text ) text = re . sub ( \"\\s\" + self . alphabets + \"[.] \" , \" \\\\ 1<prd> \" , text ) text = re . sub ( self . acronyms + \" \" + self . starters , \" \\\\ 1<stop> \\\\ 2\" , text ) text = re . sub ( self . alphabets + \"[.]\" + self . alphabets + \"[.]\" + self . alphabets + \"[.]\" , \" \\\\ 1<prd> \\\\ 2<prd> \\\\ 3<prd>\" , text ) text = re . sub ( self . alphabets + \"[.]\" + self . alphabets + \"[.]\" , \" \\\\ 1<prd> \\\\ 2<prd>\" , text ) text = re . sub ( \" \" + self . suffixes + \"[.] \" + self . starters , \" \\\\ 1<stop> \\\\ 2\" , text ) text = re . sub ( \" \" + self . suffixes + \"[.]\" , \" \\\\ 1<prd>\" , text ) text = re . sub ( self . digits + \"[.]\" + self . digits , \" \\\\ 1<prd> \\\\ 2\" , text ) # for punct in self.tokenize_map[self.dialect][self.script][\"compound_puncts\"]: # if punct in text: # text = text.replace(\".\" + punct, punct + \".\") for punct in self . tokenize_map [ \"sent_tokenize\" ][ self . dialect ][ self . script ][ \"punct_boundary\" ]: text = text . replace ( punct , punct + \"<stop>\" ) text = text . replace ( \"<prd>\" , \".\" ) sentences = text . split ( \"<stop>\" ) sentences = [ s . strip () for s in sentences if len ( s . strip ())] return sentences word_tokenize ( self , sentence , separator = '\u2581' , mwe_separator = '\u2581\u2581' , keep_form = False ) Word tokenizer Parameters: Name Type Description Default sentence str sentence or text to be tokenized required Returns: Type Description [list] [a list of words] Source code in klpt/tokenize.py def word_tokenize ( self , sentence , separator = \"\u2581\" , mwe_separator = \"\u2581\u2581\" , keep_form = False ): \"\"\"Word tokenizer Args: sentence (str): sentence or text to be tokenized Returns: [list]: [a list of words] \"\"\" # find multi-word expressions in the sentence sentence = self . mwe_tokenize ( sentence , keep_form = keep_form ) # find punctuation marks and add a space around for punct in self . tokenize_map [ \"word_tokenize\" ][ self . dialect ][ self . script ][ \"punctuation\" ]: if punct in sentence : sentence = sentence . replace ( punct , \" \" + punct + \" \" ) # print(sentence) tokens = list () # split the sentence by space and look for identifiable tokens for word in sentence . strip () . split (): if \"\u2581\u2581\" in word : # the word is previously detected as a compound word tokens . append ( word ) else : if word in self . lexicon : # check if the word exists in the lexicon tokens . append ( \"\u2581\" + word + \"\u2581\" ) else : # the word is neither a lemma nor a compound # morphological analysis by identifying affixes and clitics token_identified = False for preposition in self . morphemes [ \"prefixes\" ]: if word . startswith ( preposition ) and len ( word . split ( preposition , 1 )) > 1 : if word . split ( preposition , 1 )[ 1 ] in self . lexicon : word = \"\u2581\" . join ([ \"\" , self . morphemes [ \"prefixes\" ][ preposition ], word . split ( preposition , 1 )[ 1 ], \"\" ]) token_identified = True break elif self . mwe_tokenize ( word . split ( preposition , 1 )[ 1 ], keep_form = keep_form ) != word . split ( preposition , 1 )[ 1 ]: word = \"\u2581\" + self . morphemes [ \"prefixes\" ][ preposition ] + self . mwe_tokenize ( word . split ( preposition , 1 )[ 1 ], keep_form = keep_form ) token_identified = True break if not token_identified : for postposition in self . morphemes [ \"suffixes\" ]: if word . endswith ( postposition ) and len ( word . rpartition ( postposition )[ 0 ]): if word . rpartition ( postposition )[ 0 ] in self . lexicon : word = \"\u2581\" + word . rpartition ( postposition )[ 0 ] + \"\u2581\" + self . morphemes [ \"suffixes\" ][ postposition ] break elif self . mwe_tokenize ( word . rpartition ( postposition )[ 0 ], keep_form = keep_form ) != word . rpartition ( postposition )[ 0 ]: word = ( \"\u2581\" + self . mwe_tokenize ( word . rpartition ( postposition )[ 0 ], keep_form = keep_form ) + \"\u2581\" + self . morphemes [ \"suffixes\" ][ postposition ] + \"\u2581\" ) . replace ( \"\u2581\u2581\u2581\" , \"\u2581\u2581\" ) break tokens . append ( word ) # print(tokens) return \" \" . join ( tokens ) . replace ( \"\u2581\u2581\" , mwe_separator ) . replace ( \"\u2581\" , separator ) . split ()","title":"Tokenize"},{"location":"user-guide/tokenize/#tokenize-package","text":"This module focuses on the tokenization of both Kurmanji and Sorani dialects of Kurdish with the following functions: word_tokenize : tokenization of texts into tokens (both multi-word expressions and single-word tokens). mwe_tokenize : tokenization of texts by only taking compound forms into account sent_tokenize : tokenization of texts into sentences The module is based on the Kurdish tokenization project . Examples: >>> from klpt.tokenize import Tokenize >>> tokenizer = Tokenize(\"Kurmanji\", \"Latin\") >>> tokenizer.word_tokenize(\"ji bo fort\u00ea xwe av\u00eatin\") ['\u2581ji\u2581', 'bo', '\u2581\u2581fort\u00ea\u2012xwe\u2012av\u00eatin\u2581\u2581'] >>> tokenizer.mwe_tokenize(\"bi serok\u00ea huk\u00fbmeta her\u00eama Kurdistan\u00ea Prof. Salih re saz kir.\") 'bi serok\u00ea huk\u00fbmeta her\u00eama Kurdistan\u00ea Prof . Salih re saz kir .' >>> tokenizer_ckb = Tokenize(\"Sorani\", \"Arabic\") >>> tokenizer_ckb.word_tokenize(\"\u0628\u06d5 \u0647\u06d5\u0645\u0648\u0648 \u0647\u06d5\u0645\u0648\u0648\u0627\u0646\u06d5\u0648\u06d5 \u0695\u06ce\u06a9 \u06a9\u06d5\u0648\u062a\u0646\") ['\u2581\u0628\u06d5\u2581', '\u2581\u0647\u06d5\u0645\u0648\u0648\u2581', '\u0647\u06d5\u0645\u0648\u0648\u0627\u0646\u06d5\u0648\u06d5', '\u2581\u2581\u0695\u06ce\u06a9\u2012\u06a9\u06d5\u0648\u062a\u0646\u2581\u2581']","title":"tokenize package"},{"location":"user-guide/tokenize/#klpt.tokenize.Tokenize.mwe_tokenize","text":"Multi-word expression tokenization Parameters: Name Type Description Default sentence str sentence to be split by multi-word expressions required separator str a specific token to specify a multi-word expression. By default two \u2581 (\u2581\u2581) are used for this purpose. '\u2581\u2581' in_separator str a specific token to specify the composing parts of a multi-word expression. By default a dash - is used for this purpose. '\u2012' keep_form boolean if set to True, the original form of the multi-word expression is returned the same way provided in the input. On the other hand, if set to False, the lemma form is used where the parts are delimited by a dash \u2012, as in \"dab\u2012\u00fb\u2012ner\u00eet\" False Returns: Type Description str sentence containing d multi-word expressions using the separator Source code in klpt/tokenize.py def mwe_tokenize ( self , sentence , separator = \"\u2581\u2581\" , in_separator = \"\u2012\" , punct_marked = False , keep_form = False ): \"\"\" Multi-word expression tokenization Args: sentence (str): sentence to be split by multi-word expressions separator (str): a specific token to specify a multi-word expression. By default two \u2581 (\u2581\u2581) are used for this purpose. in_separator (str): a specific token to specify the composing parts of a multi-word expression. By default a dash - is used for this purpose. keep_form (boolean): if set to True, the original form of the multi-word expression is returned the same way provided in the input. On the other hand, if set to False, the lemma form is used where the parts are delimited by a dash \u2012, as in \"dab\u2012\u00fb\u2012ner\u00eet\" Returns: str: sentence containing d multi-word expressions using the separator \"\"\" sentence = \" \" + sentence + \" \" if not punct_marked : # find punctuation marks and add a space around for punct in self . tokenize_map [ \"word_tokenize\" ][ self . dialect ][ self . script ][ \"punctuation\" ]: if punct in sentence : sentence = sentence . replace ( punct , \" \" + punct + \" \" ) # look for compound words and delimit them by double separator for compound_lemma in self . mwe_lexicon : compound_lemma_context = \" \" + compound_lemma + \" \" if compound_lemma_context in sentence : if keep_form : sentence = sentence . replace ( compound_lemma_context , \" \u2581\u2581\" + compound_lemma + \"\u2581\u2581 \" ) else : sentence = sentence . replace ( compound_lemma_context , \" \u2581\u2581\" + compound_lemma . replace ( \"-\" , in_separator ) + \"\u2581\u2581 \" ) # check the possible word forms available for each compound lemma in the lex files, too # Note: compound forms don't have any hyphen or separator in the lex files for compound_form in self . mwe_lexicon [ compound_lemma ][ \"token_forms\" ]: compound_form_context = \" \" + compound_form + \" \" if compound_form_context in sentence : if keep_form : sentence = sentence . replace ( compound_form_context , \" \u2581\u2581\" + compound_form + \"\u2581\u2581 \" ) else : sentence = sentence . replace ( compound_form_context , \" \u2581\u2581\" + compound_lemma . replace ( \"-\" , in_separator ) + \"\u2581\u2581 \" ) # print(sentence) return sentence . replace ( \" \" , \" \" ) . replace ( \"\u2581\u2581\" , separator ) . strip ()","title":"mwe_tokenize()"},{"location":"user-guide/tokenize/#klpt.tokenize.Tokenize.sent_tokenize","text":"Sentence tokenizer Parameters: Name Type Description Default text [str] [input text to be tokenized by sentences] required Returns: Type Description [list] [a list of sentences] Source code in klpt/tokenize.py def sent_tokenize ( self , text ): \"\"\"Sentence tokenizer Args: text ([str]): [input text to be tokenized by sentences] Returns: [list]: [a list of sentences] \"\"\" text = \" \" + text + \" \" text = text . replace ( \" \\n \" , \" \" ) text = re . sub ( self . prefixes , \" \\\\ 1<prd>\" , text ) text = re . sub ( self . websites , \"<prd> \\\\ 1\" , text ) text = re . sub ( \"\\s\" + self . alphabets + \"[.] \" , \" \\\\ 1<prd> \" , text ) text = re . sub ( self . acronyms + \" \" + self . starters , \" \\\\ 1<stop> \\\\ 2\" , text ) text = re . sub ( self . alphabets + \"[.]\" + self . alphabets + \"[.]\" + self . alphabets + \"[.]\" , \" \\\\ 1<prd> \\\\ 2<prd> \\\\ 3<prd>\" , text ) text = re . sub ( self . alphabets + \"[.]\" + self . alphabets + \"[.]\" , \" \\\\ 1<prd> \\\\ 2<prd>\" , text ) text = re . sub ( \" \" + self . suffixes + \"[.] \" + self . starters , \" \\\\ 1<stop> \\\\ 2\" , text ) text = re . sub ( \" \" + self . suffixes + \"[.]\" , \" \\\\ 1<prd>\" , text ) text = re . sub ( self . digits + \"[.]\" + self . digits , \" \\\\ 1<prd> \\\\ 2\" , text ) # for punct in self.tokenize_map[self.dialect][self.script][\"compound_puncts\"]: # if punct in text: # text = text.replace(\".\" + punct, punct + \".\") for punct in self . tokenize_map [ \"sent_tokenize\" ][ self . dialect ][ self . script ][ \"punct_boundary\" ]: text = text . replace ( punct , punct + \"<stop>\" ) text = text . replace ( \"<prd>\" , \".\" ) sentences = text . split ( \"<stop>\" ) sentences = [ s . strip () for s in sentences if len ( s . strip ())] return sentences","title":"sent_tokenize()"},{"location":"user-guide/tokenize/#klpt.tokenize.Tokenize.word_tokenize","text":"Word tokenizer Parameters: Name Type Description Default sentence str sentence or text to be tokenized required Returns: Type Description [list] [a list of words] Source code in klpt/tokenize.py def word_tokenize ( self , sentence , separator = \"\u2581\" , mwe_separator = \"\u2581\u2581\" , keep_form = False ): \"\"\"Word tokenizer Args: sentence (str): sentence or text to be tokenized Returns: [list]: [a list of words] \"\"\" # find multi-word expressions in the sentence sentence = self . mwe_tokenize ( sentence , keep_form = keep_form ) # find punctuation marks and add a space around for punct in self . tokenize_map [ \"word_tokenize\" ][ self . dialect ][ self . script ][ \"punctuation\" ]: if punct in sentence : sentence = sentence . replace ( punct , \" \" + punct + \" \" ) # print(sentence) tokens = list () # split the sentence by space and look for identifiable tokens for word in sentence . strip () . split (): if \"\u2581\u2581\" in word : # the word is previously detected as a compound word tokens . append ( word ) else : if word in self . lexicon : # check if the word exists in the lexicon tokens . append ( \"\u2581\" + word + \"\u2581\" ) else : # the word is neither a lemma nor a compound # morphological analysis by identifying affixes and clitics token_identified = False for preposition in self . morphemes [ \"prefixes\" ]: if word . startswith ( preposition ) and len ( word . split ( preposition , 1 )) > 1 : if word . split ( preposition , 1 )[ 1 ] in self . lexicon : word = \"\u2581\" . join ([ \"\" , self . morphemes [ \"prefixes\" ][ preposition ], word . split ( preposition , 1 )[ 1 ], \"\" ]) token_identified = True break elif self . mwe_tokenize ( word . split ( preposition , 1 )[ 1 ], keep_form = keep_form ) != word . split ( preposition , 1 )[ 1 ]: word = \"\u2581\" + self . morphemes [ \"prefixes\" ][ preposition ] + self . mwe_tokenize ( word . split ( preposition , 1 )[ 1 ], keep_form = keep_form ) token_identified = True break if not token_identified : for postposition in self . morphemes [ \"suffixes\" ]: if word . endswith ( postposition ) and len ( word . rpartition ( postposition )[ 0 ]): if word . rpartition ( postposition )[ 0 ] in self . lexicon : word = \"\u2581\" + word . rpartition ( postposition )[ 0 ] + \"\u2581\" + self . morphemes [ \"suffixes\" ][ postposition ] break elif self . mwe_tokenize ( word . rpartition ( postposition )[ 0 ], keep_form = keep_form ) != word . rpartition ( postposition )[ 0 ]: word = ( \"\u2581\" + self . mwe_tokenize ( word . rpartition ( postposition )[ 0 ], keep_form = keep_form ) + \"\u2581\" + self . morphemes [ \"suffixes\" ][ postposition ] + \"\u2581\" ) . replace ( \"\u2581\u2581\u2581\" , \"\u2581\u2581\" ) break tokens . append ( word ) # print(tokens) return \" \" . join ( tokens ) . replace ( \"\u2581\u2581\" , mwe_separator ) . replace ( \"\u2581\" , separator ) . split ()","title":"word_tokenize()"},{"location":"user-guide/transliterate/","text":"transliterate package This module aims at transliterating one script of Kurdish into another one. Currently, only the Latin-based and the Arabic-based scripts of Sorani and Kurmanji are supported. The main function in this module is transliterate() which also takes care of detecting the correct form of double-usage graphemes, namely \u0648 \u2194 w/u and \u06cc \u2194 \u00ee/y. In some specific occasions, it can also predict the placement of the missing i (also known as Bizroke/\u0628\u0632\u0631\u06c6\u06a9\u06d5 ). The module is based on the Kurdish transliteration project . Examples: >>> from klpt.transliterate import Transliterate >>> transliterate = Transliterate(\"Kurmanji\", \"Latin\", target_script=\"Arabic\") >>> transliterate.transliterate(\"rojhilata nav\u00een\") '\u0631\u06c6\u0698\u0647\u0644\u0627\u062a\u0627 \u0646\u0627\u06a4\u06cc\u0646' >>> transliterate_ckb = Transliterate(\"Sorani\", \"Arabic\", target_script=\"Latin\") >>> transliterate_ckb.transliterate(\"\u0644\u06d5 \u0648\u06b5\u0627\u062a\u06d5\u06a9\u0627\u0646\u06cc \u062f\u06cc\u06a9\u06d5\u062f\u0627\") 'le wi\u0142atekan\u00ee d\u00eekeda' __init__ ( self , dialect , script , target_script , unknown = '\ufffd' , numeral = 'Latin' ) special Initializing using a Configuration object To do: - \"\u0644\u06d5 \u0626\u06cc\u0633\u067e\u0627\u0646\u06cc\u0627 \u0698\u0646\u0627\u0646 \u0644\u06d5 \u062f\u0698\u06cc \u2018patriarkavirus\u2019 \u0695\u06ce\u067e\u06ce\u0648\u0627\u0646\u06cc\u0627\u0646 \u06a9\u0631\u062f\": \"le \u00eespanya jinan le dij\u00ee \u2018patriarkavirus\u2019 \u0159\u00eap\u00eawanyan kird\" - \"eger\u00e7\u00ee damezrandn\u00ee r\u00eakxrawe kurd\u00eeyekan her r\u00eap\u00eanedraw mab\u00fbnewe Inz\u00eebat.\": \"\u0626\u06d5\u06af\u06d5\u0631\u0686\u06cc \u062f\u0627\u0645\u06d5\u0632\u0631\u0627\u0646\u062f\u0646\u06cc \u0695\u06ce\u06a9\u062e\u0631\u0627\u0648\u06d5 \u06a9\u0648\u0631\u062f\u06cc\u06cc\u06d5\u06a9\u0627\u0646 \u0647\u06d5\u0631 \u0631\u06ce\u067e\u06ce\u0646\u06d5\u062f\u0631\u0627\u0648 \u0645\u0627\u0628\u0648\u0648\u0646\u06d5\u0648\u06d5 \u0626\u0646\u0632\u06cc\u0628\u0627\u062a.\", Parameters: Name Type Description Default mode [type] [description] required unknown str [description]. Defaults to \"\ufffd\". '\ufffd' numeral str [description]. Defaults to \"Latin\". Modifiable only if the source script is in Arabic. Otherwise, the Default value will be Latin. 'Latin' Exceptions: Type Description ValueError [description] ValueError [description] Source code in klpt/transliterate.py def __init__ ( self , dialect , script , target_script , unknown = \"\ufffd\" , numeral = \"Latin\" ): \"\"\"Initializing using a Configuration object To do: - \"\u0644\u06d5 \u0626\u06cc\u0633\u067e\u0627\u0646\u06cc\u0627 \u0698\u0646\u0627\u0646 \u0644\u06d5 \u062f\u0698\u06cc \u2018patriarkavirus\u2019 \u0695\u06ce\u067e\u06ce\u0648\u0627\u0646\u06cc\u0627\u0646 \u06a9\u0631\u062f\": \"le \u00eespanya jinan le dij\u00ee \u2018patriarkavirus\u2019 \u0159\u00eap\u00eawanyan kird\" - \"eger\u00e7\u00ee damezrandn\u00ee r\u00eakxrawe kurd\u00eeyekan her r\u00eap\u00eanedraw mab\u00fbnewe Inz\u00eebat.\": \"\u0626\u06d5\u06af\u06d5\u0631\u0686\u06cc \u062f\u0627\u0645\u06d5\u0632\u0631\u0627\u0646\u062f\u0646\u06cc \u0695\u06ce\u06a9\u062e\u0631\u0627\u0648\u06d5 \u06a9\u0648\u0631\u062f\u06cc\u06cc\u06d5\u06a9\u0627\u0646 \u0647\u06d5\u0631 \u0631\u06ce\u067e\u06ce\u0646\u06d5\u062f\u0631\u0627\u0648 \u0645\u0627\u0628\u0648\u0648\u0646\u06d5\u0648\u06d5 \u0626\u0646\u0632\u06cc\u0628\u0627\u062a.\", Args: mode ([type]): [description] unknown (str, optional): [description]. Defaults to \"\ufffd\". numeral (str, optional): [description]. Defaults to \"Latin\". Modifiable only if the source script is in Arabic. Otherwise, the Default value will be Latin. Raises: ValueError: [description] ValueError: [description] \"\"\" # with open(\"data/default-options.json\") as f: # options = json.load(f) self . UNKNOWN = \"\ufffd\" with open ( klpt . get_data ( \"data/wergor.json\" ), encoding = \"utf-8\" ) as f : self . wergor_configurations = json . load ( f ) with open ( klpt . get_data ( \"data/preprocess_map.json\" ), encoding = \"utf-8\" ) as f : self . preprocess_map = json . load ( f )[ \"normalizer\" ] configuration = Configuration ({ \"dialect\" : dialect , \"script\" : script , \"numeral\" : numeral , \"target_script\" : target_script , \"unknown\" : unknown }) # self.preprocess_map = object.preprocess_map[\"normalizer\"] self . dialect = configuration . dialect self . script = configuration . script self . numeral = configuration . numeral self . mode = configuration . mode self . target_script = configuration . target_script self . user_UNKNOWN = configuration . user_UNKNOWN # self.mode = mode # if mode==\"arabic_to_latin\": # target_script = \"Latin\" # elif mode==\"latin_to_arabic\": # target_script = \"Arabic\" # else: # raise ValueError(f'Unknown transliteration option. Available options: {options[\"transliterator\"]}') # if len(unknown): # self.user_UNKNOWN = unknown # else: # raise ValueError(f'Unknown unknown tag. Select a non-empty token (e.g. <UNK>.') self . characters_mapping = self . wergor_configurations [ \"characters_mapping\" ] self . digits_mapping = self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] self . digits_mapping_all = list ( set ( list ( self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] . keys ()) + list ( self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] . values ()))) self . punctuation_mapping = self . wergor_configurations [ \"punctuation\" ][ self . target_script ] self . punctuation_mapping_all = list ( set ( list ( self . wergor_configurations [ \"punctuation\" ][ self . target_script ] . keys ()) + list ( self . wergor_configurations [ \"punctuation\" ][ self . target_script ] . values ()))) # self.tricky_characters = self.wergor_configurations[\"characters_mapping\"] self . wy_mappings = self . wergor_configurations [ \"wy_mappings\" ] self . hemze = self . wergor_configurations [ \"hemze\" ] self . bizroke = self . wergor_configurations [ \"bizroke\" ] self . uw_iy_forms = self . wergor_configurations [ \"uw_iy_forms\" ] self . target_char = self . wergor_configurations [ \"target_char\" ] self . arabic_vowels = self . wergor_configurations [ \"arabic_vowels\" ] self . arabic_cons = self . wergor_configurations [ \"arabic_cons\" ] self . latin_vowels = self . wergor_configurations [ \"latin_vowels\" ] self . latin_cons = self . wergor_configurations [ \"latin_cons\" ] self . characters_pack = { \"arabic_to_latin\" : self . characters_mapping . values (), \"latin_to_arabic\" : self . characters_mapping . keys ()} if self . target_script == \"Arabic\" : self . prep = Preprocess ( \"Sorani\" , \"Latin\" , numeral = self . numeral ) else : self . prep = Preprocess ( \"Sorani\" , \"Latin\" , numeral = \"Latin\" ) arabic_to_latin ( self , char ) Mapping Arabic-based characters to the Latin-based equivalents Source code in klpt/transliterate.py def arabic_to_latin ( self , char ): \"\"\"Mapping Arabic-based characters to the Latin-based equivalents\"\"\" if char != \"\" : if char in list ( self . characters_mapping . values ()): return list ( self . characters_mapping . keys ())[ list ( self . characters_mapping . values ()) . index ( char )] elif char in self . punctuation_mapping : return self . punctuation_mapping [ char ] elif char in self . punctuation_mapping : return self . punctuation_mapping [ char ] return char bizroke_finder ( self , word ) Detection of the \"i\" character in the Arabic-based script. Incomplete version. Source code in klpt/transliterate.py def bizroke_finder ( self , word ): \"\"\"Detection of the \"i\" character in the Arabic-based script. Incomplete version.\"\"\" word = list ( word ) if len ( word ) > 2 and word [ 0 ] in self . latin_cons and word [ 1 ] in self . latin_cons and word [ 1 ] != \"w\" and word [ 1 ] != \"y\" : word . insert ( 1 , \"i\" ) return \"\" . join ( word ) latin_to_arabic ( self , char ) Mapping Latin-based characters to the Arabic-based equivalents Source code in klpt/transliterate.py def latin_to_arabic ( self , char ): \"\"\"Mapping Latin-based characters to the Arabic-based equivalents\"\"\" # check if the character is in upper case mapped_char = \"\" if char . lower () != \"\" : if char . lower () in self . wy_mappings . keys (): mapped_char = self . wy_mappings [ char . lower ()] elif char . lower () in self . characters_mapping . keys (): mapped_char = self . characters_mapping [ char . lower ()] elif char . lower () in self . punctuation_mapping : mapped_char = self . punctuation_mapping [ char . lower ()] # elif char.lower() in self.digits_mapping.values(): # mapped_char = self.digits_mapping.keys()[self.digits_mapping.values().index(char.lower())] if len ( mapped_char ): if char . isupper (): return mapped_char . upper () return mapped_char else : return char preprocessor ( self , word ) Preprocessing by normalizing text encoding and removing embedding characters Source code in klpt/transliterate.py def preprocessor ( self , word ): \"\"\"Preprocessing by normalizing text encoding and removing embedding characters\"\"\" # replace this by the normalization part word = list ( word . replace ( ' \\u202b ' , \"\" ) . replace ( ' \\u202c ' , \"\" ) . replace ( ' \\u202a ' , \"\" ) . replace ( u \"\u0648\u0648\" , \"\u00fb\" ) . replace ( \" \\u200c \" , \"\" ) . replace ( \"\u0640\" , \"\" )) # for char_index in range(len(word)): # if(word[char_index] in self.tricky_characters.keys()): # word[char_index] = self.tricky_characters[word[char_index]] return \"\" . join ( word ) syllable_detector ( self , word ) Detection of the syllable based on the given pattern. May be used for transcription applications. Source code in klpt/transliterate.py def syllable_detector ( self , word ): \"\"\"Detection of the syllable based on the given pattern. May be used for transcription applications.\"\"\" syllable_templates = [ \"V\" , \"VC\" , \"VCC\" , \"CV\" , \"CVC\" , \"CVCCC\" ] CV_converted_list = \"\" for char in word : if char in self . latin_vowels : CV_converted_list += \"V\" else : CV_converted_list += \"C\" syllables = list () for i in range ( 1 , len ( CV_converted_list )): syllable_templates_permutated = [ p for p in itertools . product ( syllable_templates , repeat = i )] for syl in syllable_templates_permutated : if len ( \"\" . join ( syl )) == len ( CV_converted_list ): if CV_converted_list == \"\" . join ( syl ) and \"VV\" not in \"\" . join ( syl ): syllables . append ( syl ) return syllables to_pieces ( self , token ) Given a token, find other segments composed of numbers and punctuation marks not seperated by space \u2581 Source code in klpt/transliterate.py def to_pieces ( self , token ): \"\"\"Given a token, find other segments composed of numbers and punctuation marks not seperated by space \u2581\"\"\" tokens_dict = dict () flag = False # True if a token is a \\w i = 0 for char_index in range ( len ( token )): if token [ char_index ] in self . digits_mapping_all or token [ char_index ] in self . punctuation_mapping_all : tokens_dict [ char_index ] = token [ char_index ] flag = False i = 0 elif token [ char_index ] in self . characters_pack [ self . mode ] or \\ token [ char_index ] in self . target_char or \\ token [ char_index ] == self . hemze or token [ char_index ] . lower () == self . bizroke : if flag : tokens_dict [ char_index - i ] = tokens_dict [ char_index - i ] + token [ char_index ] else : tokens_dict [ char_index ] = token [ char_index ] flag = True i += 1 else : tokens_dict [ char_index ] = self . UNKNOWN return tokens_dict transliterate ( self , text ) The main method of the class: - find word boundaries by splitting it using spaces and then retrieve words mixed with other characters (without space) - map characters - detect double-usage characters w/u and y/\u00ee - find possible position of Bizroke (to be completed - 2017) Notice: text format should not be changed at all (no lower case, no style replacement , etc.). If the source and the target scripts are identical, the input text should be returned without any further processing. Source code in klpt/transliterate.py def transliterate ( self , text ): \"\"\"The main method of the class: - find word boundaries by splitting it using spaces and then retrieve words mixed with other characters (without space) - map characters - detect double-usage characters w/u and y/\u00ee - find possible position of Bizroke (to be completed - 2017) Notice: text format should not be changed at all (no lower case, no style replacement \\t, \\n etc.). If the source and the target scripts are identical, the input text should be returned without any further processing. \"\"\" text = self . prep . unify_numerals ( text ) . split ( \" \\n \" ) transliterated_text = list () for line in text : transliterated_line = list () for token in line . split (): trans_token = \"\" # try: token = self . preprocessor ( token ) # This is not correct as the capital letter should be kept the way it is given. tokens_dict = self . to_pieces ( token ) # Transliterate words for token_key in tokens_dict : if len ( tokens_dict [ token_key ]): word = tokens_dict [ token_key ] if self . mode == \"arabic_to_latin\" : # w/y detection based on the priority in \"word\" for char in word : if char in self . target_char : word = self . uw_iy_Detector ( word , char ) if word [ 0 ] == self . hemze and word [ 1 ] in self . arabic_vowels : word = word [ 1 :] word = list ( word ) for char_index in range ( len ( word )): word [ char_index ] = self . arabic_to_latin ( word [ char_index ]) word = \"\" . join ( word ) word = self . bizroke_finder ( word ) elif self . mode == \"latin_to_arabic\" : if len ( word ): word = list ( word ) for char_index in range ( len ( word )): word [ char_index ] = self . latin_to_arabic ( word [ char_index ]) if word [ 0 ] in self . arabic_vowels or word [ 0 ] . lower () == self . bizroke : word . insert ( 0 , self . hemze ) word = \"\" . join ( word ) . replace ( \"\u00fb\" , \"\u0648\u0648\" ) . replace ( self . bizroke . lower (), \"\" ) . replace ( self . bizroke . upper (), \"\" ) # else: # return self.UNKNOWN trans_token = trans_token + word transliterated_line . append ( trans_token ) transliterated_text . append ( \" \" . join ( transliterated_line ) . replace ( u \" w \" , u \" \u00fb \" )) # standardize the output # replace UNKOWN by the user's choice if self . user_UNKNOWN != self . UNKNOWN : return \" \\n \" . join ( transliterated_text ) . replace ( self . UNKNOWN , self . user_UNKNOWN ) else : return \" \\n \" . join ( transliterated_text ) uw_iy_Detector ( self , word , target_char ) Detection of \"\u0648\" and \"\u06cc\" in the Arabic-based script Source code in klpt/transliterate.py def uw_iy_Detector ( self , word , target_char ): \"\"\"Detection of \"\u0648\" and \"\u06cc\" in the Arabic-based script\"\"\" word = list ( word ) if target_char == \"\u0648\" : dic_index = 1 else : dic_index = 0 if word == target_char : word = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : for index in range ( len ( word )): if word [ index ] == self . hemze and word [ index + 1 ] == target_char : word [ index + 1 ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] index += 1 else : if word [ index ] == target_char : if index == 0 : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : if word [ index - 1 ] in self . arabic_vowels : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : if index + 1 < len ( word ): if word [ index + 1 ] in self . arabic_vowels : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : word [ index ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] else : word [ index ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] word = \"\" . join ( word ) . replace ( self . hemze + self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ], self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ]) return word","title":"Transliterate"},{"location":"user-guide/transliterate/#transliterate-package","text":"This module aims at transliterating one script of Kurdish into another one. Currently, only the Latin-based and the Arabic-based scripts of Sorani and Kurmanji are supported. The main function in this module is transliterate() which also takes care of detecting the correct form of double-usage graphemes, namely \u0648 \u2194 w/u and \u06cc \u2194 \u00ee/y. In some specific occasions, it can also predict the placement of the missing i (also known as Bizroke/\u0628\u0632\u0631\u06c6\u06a9\u06d5 ). The module is based on the Kurdish transliteration project . Examples: >>> from klpt.transliterate import Transliterate >>> transliterate = Transliterate(\"Kurmanji\", \"Latin\", target_script=\"Arabic\") >>> transliterate.transliterate(\"rojhilata nav\u00een\") '\u0631\u06c6\u0698\u0647\u0644\u0627\u062a\u0627 \u0646\u0627\u06a4\u06cc\u0646' >>> transliterate_ckb = Transliterate(\"Sorani\", \"Arabic\", target_script=\"Latin\") >>> transliterate_ckb.transliterate(\"\u0644\u06d5 \u0648\u06b5\u0627\u062a\u06d5\u06a9\u0627\u0646\u06cc \u062f\u06cc\u06a9\u06d5\u062f\u0627\") 'le wi\u0142atekan\u00ee d\u00eekeda'","title":"transliterate package"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.__init__","text":"Initializing using a Configuration object To do: - \"\u0644\u06d5 \u0626\u06cc\u0633\u067e\u0627\u0646\u06cc\u0627 \u0698\u0646\u0627\u0646 \u0644\u06d5 \u062f\u0698\u06cc \u2018patriarkavirus\u2019 \u0695\u06ce\u067e\u06ce\u0648\u0627\u0646\u06cc\u0627\u0646 \u06a9\u0631\u062f\": \"le \u00eespanya jinan le dij\u00ee \u2018patriarkavirus\u2019 \u0159\u00eap\u00eawanyan kird\" - \"eger\u00e7\u00ee damezrandn\u00ee r\u00eakxrawe kurd\u00eeyekan her r\u00eap\u00eanedraw mab\u00fbnewe Inz\u00eebat.\": \"\u0626\u06d5\u06af\u06d5\u0631\u0686\u06cc \u062f\u0627\u0645\u06d5\u0632\u0631\u0627\u0646\u062f\u0646\u06cc \u0695\u06ce\u06a9\u062e\u0631\u0627\u0648\u06d5 \u06a9\u0648\u0631\u062f\u06cc\u06cc\u06d5\u06a9\u0627\u0646 \u0647\u06d5\u0631 \u0631\u06ce\u067e\u06ce\u0646\u06d5\u062f\u0631\u0627\u0648 \u0645\u0627\u0628\u0648\u0648\u0646\u06d5\u0648\u06d5 \u0626\u0646\u0632\u06cc\u0628\u0627\u062a.\", Parameters: Name Type Description Default mode [type] [description] required unknown str [description]. Defaults to \"\ufffd\". '\ufffd' numeral str [description]. Defaults to \"Latin\". Modifiable only if the source script is in Arabic. Otherwise, the Default value will be Latin. 'Latin' Exceptions: Type Description ValueError [description] ValueError [description] Source code in klpt/transliterate.py def __init__ ( self , dialect , script , target_script , unknown = \"\ufffd\" , numeral = \"Latin\" ): \"\"\"Initializing using a Configuration object To do: - \"\u0644\u06d5 \u0626\u06cc\u0633\u067e\u0627\u0646\u06cc\u0627 \u0698\u0646\u0627\u0646 \u0644\u06d5 \u062f\u0698\u06cc \u2018patriarkavirus\u2019 \u0695\u06ce\u067e\u06ce\u0648\u0627\u0646\u06cc\u0627\u0646 \u06a9\u0631\u062f\": \"le \u00eespanya jinan le dij\u00ee \u2018patriarkavirus\u2019 \u0159\u00eap\u00eawanyan kird\" - \"eger\u00e7\u00ee damezrandn\u00ee r\u00eakxrawe kurd\u00eeyekan her r\u00eap\u00eanedraw mab\u00fbnewe Inz\u00eebat.\": \"\u0626\u06d5\u06af\u06d5\u0631\u0686\u06cc \u062f\u0627\u0645\u06d5\u0632\u0631\u0627\u0646\u062f\u0646\u06cc \u0695\u06ce\u06a9\u062e\u0631\u0627\u0648\u06d5 \u06a9\u0648\u0631\u062f\u06cc\u06cc\u06d5\u06a9\u0627\u0646 \u0647\u06d5\u0631 \u0631\u06ce\u067e\u06ce\u0646\u06d5\u062f\u0631\u0627\u0648 \u0645\u0627\u0628\u0648\u0648\u0646\u06d5\u0648\u06d5 \u0626\u0646\u0632\u06cc\u0628\u0627\u062a.\", Args: mode ([type]): [description] unknown (str, optional): [description]. Defaults to \"\ufffd\". numeral (str, optional): [description]. Defaults to \"Latin\". Modifiable only if the source script is in Arabic. Otherwise, the Default value will be Latin. Raises: ValueError: [description] ValueError: [description] \"\"\" # with open(\"data/default-options.json\") as f: # options = json.load(f) self . UNKNOWN = \"\ufffd\" with open ( klpt . get_data ( \"data/wergor.json\" ), encoding = \"utf-8\" ) as f : self . wergor_configurations = json . load ( f ) with open ( klpt . get_data ( \"data/preprocess_map.json\" ), encoding = \"utf-8\" ) as f : self . preprocess_map = json . load ( f )[ \"normalizer\" ] configuration = Configuration ({ \"dialect\" : dialect , \"script\" : script , \"numeral\" : numeral , \"target_script\" : target_script , \"unknown\" : unknown }) # self.preprocess_map = object.preprocess_map[\"normalizer\"] self . dialect = configuration . dialect self . script = configuration . script self . numeral = configuration . numeral self . mode = configuration . mode self . target_script = configuration . target_script self . user_UNKNOWN = configuration . user_UNKNOWN # self.mode = mode # if mode==\"arabic_to_latin\": # target_script = \"Latin\" # elif mode==\"latin_to_arabic\": # target_script = \"Arabic\" # else: # raise ValueError(f'Unknown transliteration option. Available options: {options[\"transliterator\"]}') # if len(unknown): # self.user_UNKNOWN = unknown # else: # raise ValueError(f'Unknown unknown tag. Select a non-empty token (e.g. <UNK>.') self . characters_mapping = self . wergor_configurations [ \"characters_mapping\" ] self . digits_mapping = self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] self . digits_mapping_all = list ( set ( list ( self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] . keys ()) + list ( self . preprocess_map [ \"universal\" ][ \"numerals\" ][ self . target_script ] . values ()))) self . punctuation_mapping = self . wergor_configurations [ \"punctuation\" ][ self . target_script ] self . punctuation_mapping_all = list ( set ( list ( self . wergor_configurations [ \"punctuation\" ][ self . target_script ] . keys ()) + list ( self . wergor_configurations [ \"punctuation\" ][ self . target_script ] . values ()))) # self.tricky_characters = self.wergor_configurations[\"characters_mapping\"] self . wy_mappings = self . wergor_configurations [ \"wy_mappings\" ] self . hemze = self . wergor_configurations [ \"hemze\" ] self . bizroke = self . wergor_configurations [ \"bizroke\" ] self . uw_iy_forms = self . wergor_configurations [ \"uw_iy_forms\" ] self . target_char = self . wergor_configurations [ \"target_char\" ] self . arabic_vowels = self . wergor_configurations [ \"arabic_vowels\" ] self . arabic_cons = self . wergor_configurations [ \"arabic_cons\" ] self . latin_vowels = self . wergor_configurations [ \"latin_vowels\" ] self . latin_cons = self . wergor_configurations [ \"latin_cons\" ] self . characters_pack = { \"arabic_to_latin\" : self . characters_mapping . values (), \"latin_to_arabic\" : self . characters_mapping . keys ()} if self . target_script == \"Arabic\" : self . prep = Preprocess ( \"Sorani\" , \"Latin\" , numeral = self . numeral ) else : self . prep = Preprocess ( \"Sorani\" , \"Latin\" , numeral = \"Latin\" )","title":"__init__()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.arabic_to_latin","text":"Mapping Arabic-based characters to the Latin-based equivalents Source code in klpt/transliterate.py def arabic_to_latin ( self , char ): \"\"\"Mapping Arabic-based characters to the Latin-based equivalents\"\"\" if char != \"\" : if char in list ( self . characters_mapping . values ()): return list ( self . characters_mapping . keys ())[ list ( self . characters_mapping . values ()) . index ( char )] elif char in self . punctuation_mapping : return self . punctuation_mapping [ char ] elif char in self . punctuation_mapping : return self . punctuation_mapping [ char ] return char","title":"arabic_to_latin()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.bizroke_finder","text":"Detection of the \"i\" character in the Arabic-based script. Incomplete version. Source code in klpt/transliterate.py def bizroke_finder ( self , word ): \"\"\"Detection of the \"i\" character in the Arabic-based script. Incomplete version.\"\"\" word = list ( word ) if len ( word ) > 2 and word [ 0 ] in self . latin_cons and word [ 1 ] in self . latin_cons and word [ 1 ] != \"w\" and word [ 1 ] != \"y\" : word . insert ( 1 , \"i\" ) return \"\" . join ( word )","title":"bizroke_finder()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.latin_to_arabic","text":"Mapping Latin-based characters to the Arabic-based equivalents Source code in klpt/transliterate.py def latin_to_arabic ( self , char ): \"\"\"Mapping Latin-based characters to the Arabic-based equivalents\"\"\" # check if the character is in upper case mapped_char = \"\" if char . lower () != \"\" : if char . lower () in self . wy_mappings . keys (): mapped_char = self . wy_mappings [ char . lower ()] elif char . lower () in self . characters_mapping . keys (): mapped_char = self . characters_mapping [ char . lower ()] elif char . lower () in self . punctuation_mapping : mapped_char = self . punctuation_mapping [ char . lower ()] # elif char.lower() in self.digits_mapping.values(): # mapped_char = self.digits_mapping.keys()[self.digits_mapping.values().index(char.lower())] if len ( mapped_char ): if char . isupper (): return mapped_char . upper () return mapped_char else : return char","title":"latin_to_arabic()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.preprocessor","text":"Preprocessing by normalizing text encoding and removing embedding characters Source code in klpt/transliterate.py def preprocessor ( self , word ): \"\"\"Preprocessing by normalizing text encoding and removing embedding characters\"\"\" # replace this by the normalization part word = list ( word . replace ( ' \\u202b ' , \"\" ) . replace ( ' \\u202c ' , \"\" ) . replace ( ' \\u202a ' , \"\" ) . replace ( u \"\u0648\u0648\" , \"\u00fb\" ) . replace ( \" \\u200c \" , \"\" ) . replace ( \"\u0640\" , \"\" )) # for char_index in range(len(word)): # if(word[char_index] in self.tricky_characters.keys()): # word[char_index] = self.tricky_characters[word[char_index]] return \"\" . join ( word )","title":"preprocessor()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.syllable_detector","text":"Detection of the syllable based on the given pattern. May be used for transcription applications. Source code in klpt/transliterate.py def syllable_detector ( self , word ): \"\"\"Detection of the syllable based on the given pattern. May be used for transcription applications.\"\"\" syllable_templates = [ \"V\" , \"VC\" , \"VCC\" , \"CV\" , \"CVC\" , \"CVCCC\" ] CV_converted_list = \"\" for char in word : if char in self . latin_vowels : CV_converted_list += \"V\" else : CV_converted_list += \"C\" syllables = list () for i in range ( 1 , len ( CV_converted_list )): syllable_templates_permutated = [ p for p in itertools . product ( syllable_templates , repeat = i )] for syl in syllable_templates_permutated : if len ( \"\" . join ( syl )) == len ( CV_converted_list ): if CV_converted_list == \"\" . join ( syl ) and \"VV\" not in \"\" . join ( syl ): syllables . append ( syl ) return syllables","title":"syllable_detector()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.to_pieces","text":"Given a token, find other segments composed of numbers and punctuation marks not seperated by space \u2581 Source code in klpt/transliterate.py def to_pieces ( self , token ): \"\"\"Given a token, find other segments composed of numbers and punctuation marks not seperated by space \u2581\"\"\" tokens_dict = dict () flag = False # True if a token is a \\w i = 0 for char_index in range ( len ( token )): if token [ char_index ] in self . digits_mapping_all or token [ char_index ] in self . punctuation_mapping_all : tokens_dict [ char_index ] = token [ char_index ] flag = False i = 0 elif token [ char_index ] in self . characters_pack [ self . mode ] or \\ token [ char_index ] in self . target_char or \\ token [ char_index ] == self . hemze or token [ char_index ] . lower () == self . bizroke : if flag : tokens_dict [ char_index - i ] = tokens_dict [ char_index - i ] + token [ char_index ] else : tokens_dict [ char_index ] = token [ char_index ] flag = True i += 1 else : tokens_dict [ char_index ] = self . UNKNOWN return tokens_dict","title":"to_pieces()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.transliterate","text":"The main method of the class: - find word boundaries by splitting it using spaces and then retrieve words mixed with other characters (without space) - map characters - detect double-usage characters w/u and y/\u00ee - find possible position of Bizroke (to be completed - 2017) Notice: text format should not be changed at all (no lower case, no style replacement , etc.). If the source and the target scripts are identical, the input text should be returned without any further processing. Source code in klpt/transliterate.py def transliterate ( self , text ): \"\"\"The main method of the class: - find word boundaries by splitting it using spaces and then retrieve words mixed with other characters (without space) - map characters - detect double-usage characters w/u and y/\u00ee - find possible position of Bizroke (to be completed - 2017) Notice: text format should not be changed at all (no lower case, no style replacement \\t, \\n etc.). If the source and the target scripts are identical, the input text should be returned without any further processing. \"\"\" text = self . prep . unify_numerals ( text ) . split ( \" \\n \" ) transliterated_text = list () for line in text : transliterated_line = list () for token in line . split (): trans_token = \"\" # try: token = self . preprocessor ( token ) # This is not correct as the capital letter should be kept the way it is given. tokens_dict = self . to_pieces ( token ) # Transliterate words for token_key in tokens_dict : if len ( tokens_dict [ token_key ]): word = tokens_dict [ token_key ] if self . mode == \"arabic_to_latin\" : # w/y detection based on the priority in \"word\" for char in word : if char in self . target_char : word = self . uw_iy_Detector ( word , char ) if word [ 0 ] == self . hemze and word [ 1 ] in self . arabic_vowels : word = word [ 1 :] word = list ( word ) for char_index in range ( len ( word )): word [ char_index ] = self . arabic_to_latin ( word [ char_index ]) word = \"\" . join ( word ) word = self . bizroke_finder ( word ) elif self . mode == \"latin_to_arabic\" : if len ( word ): word = list ( word ) for char_index in range ( len ( word )): word [ char_index ] = self . latin_to_arabic ( word [ char_index ]) if word [ 0 ] in self . arabic_vowels or word [ 0 ] . lower () == self . bizroke : word . insert ( 0 , self . hemze ) word = \"\" . join ( word ) . replace ( \"\u00fb\" , \"\u0648\u0648\" ) . replace ( self . bizroke . lower (), \"\" ) . replace ( self . bizroke . upper (), \"\" ) # else: # return self.UNKNOWN trans_token = trans_token + word transliterated_line . append ( trans_token ) transliterated_text . append ( \" \" . join ( transliterated_line ) . replace ( u \" w \" , u \" \u00fb \" )) # standardize the output # replace UNKOWN by the user's choice if self . user_UNKNOWN != self . UNKNOWN : return \" \\n \" . join ( transliterated_text ) . replace ( self . UNKNOWN , self . user_UNKNOWN ) else : return \" \\n \" . join ( transliterated_text )","title":"transliterate()"},{"location":"user-guide/transliterate/#klpt.transliterate.Transliterate.uw_iy_Detector","text":"Detection of \"\u0648\" and \"\u06cc\" in the Arabic-based script Source code in klpt/transliterate.py def uw_iy_Detector ( self , word , target_char ): \"\"\"Detection of \"\u0648\" and \"\u06cc\" in the Arabic-based script\"\"\" word = list ( word ) if target_char == \"\u0648\" : dic_index = 1 else : dic_index = 0 if word == target_char : word = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : for index in range ( len ( word )): if word [ index ] == self . hemze and word [ index + 1 ] == target_char : word [ index + 1 ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] index += 1 else : if word [ index ] == target_char : if index == 0 : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : if word [ index - 1 ] in self . arabic_vowels : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : if index + 1 < len ( word ): if word [ index + 1 ] in self . arabic_vowels : word [ index ] = self . uw_iy_forms [ \"target_char_cons\" ][ dic_index ] else : word [ index ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] else : word [ index ] = self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ] word = \"\" . join ( word ) . replace ( self . hemze + self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ], self . uw_iy_forms [ \"target_char_vowel\" ][ dic_index ]) return word","title":"uw_iy_Detector()"}]}